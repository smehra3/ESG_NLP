{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESG NLP Research.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "icQiRZyugGSX",
        "WTpyzvTA27jU",
        "x-gkWfj10XdW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK5QHfj7WEjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5416b88b-b02a-474d-fcb7-c882023cf110"
      },
      "source": [
        "!apt-get install python3-pypdf2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  python3-pypdf2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 49.2 kB of archives.\n",
            "After this operation, 263 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-pypdf2 all 1.26.0-2 [49.2 kB]\n",
            "Fetched 49.2 kB in 0s (102 kB/s)\n",
            "Selecting previously unselected package python3-pypdf2.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-pypdf2_1.26.0-2_all.deb ...\n",
            "Unpacking python3-pypdf2 (1.26.0-2) ...\n",
            "Setting up python3-pypdf2 (1.26.0-2) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69-nrsujfHWQ"
      },
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T_fHxpd2pyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f1df94-6bd6-46e9-8597-a949bc83658a"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/fd/8a81047bbd9fa134a3f27e12937d2a487bd49d353a038916a5d7ed4e5543/sentence-transformers-2.0.0.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.5MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 32.8MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/35/03/071adc023c0a7e540cf4652fa9cad13ab32e6ae469bf0cc0262045244812/huggingface_hub-0.0.13-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.5.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 39.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.13)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-cp37-none-any.whl size=126711 sha256=2ef7accb137cfd6814ba5746cfdbb007c012de9033629bf351b441cd87386bf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/d2/98/d191289a877a34c68aa67e05179521e060f96394a3e9336be6\n",
            "Successfully built sentence-transformers\n",
            "\u001b[31mERROR: transformers 4.8.2 has requirement huggingface-hub==0.0.12, but you'll have huggingface-hub 0.0.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.0.13 sacremoses-0.0.45 sentence-transformers-2.0.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC_Sy4WogB31"
      },
      "source": [
        "from google.colab import auth\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "import pandas as pd\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "import PyPDF2\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P0DrUrF-AP_",
        "outputId": "79c47e90-d8c6-46b5-dbdf-7a2e4e20c849"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icQiRZyugGSX"
      },
      "source": [
        "# Earnings Call Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h25uu1jOPhGl"
      },
      "source": [
        "symbols = ['AAPL', 'MSFT', 'GOOGL', 'XOM', 'CVX']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwDh9jXdu1wb"
      },
      "source": [
        "def create_earnings_calls_list(symbols):\n",
        "  earnings_calls = []\n",
        "\n",
        "  for symbol in symbols:\n",
        "\n",
        "    #api to get all transcripts per the symbol\n",
        "    symbol_api_req = requests.get('https://finnhub.io/api/v1/stock/transcripts?symbol='+symbol+'&token=c2uvbbqad3i9mrpuqq00')\n",
        "    \n",
        "    for t in symbol_api_req.json().get(\"transcripts\"):\n",
        "      #api to get speech text, speaker, and other details per transcript\n",
        "      transcript_api_req = requests.get('https://finnhub.io/api/v1/stock/transcripts?id='+t.get(\"id\")+'&token=c2uvbbqad3i9mrpuqq00')\n",
        "      resulting_json = transcript_api_req.json()\n",
        "      \n",
        "      #build name-designation loopup for each transcript\n",
        "      name_designation_lookup = {}\n",
        "      if(\"participant\" in resulting_json.keys()):\n",
        "        for p in resulting_json.get(\"participant\"):\n",
        "          name_designation_lookup[p.get(\"name\")] = p.get(\"description\") \n",
        "      #build dictionary to store speech and speaker details\n",
        "      if(\"transcript\" in resulting_json.keys()):\n",
        "        for transcript in resulting_json.get(\"transcript\"):\n",
        "          if (transcript.get(\"name\")!=\"Operator\"):\n",
        "            #speeches = []\n",
        "            #speeches.append({})\n",
        "          #save every transcript and speeches per transcript with symbol key in earnings_calls_dictionary\n",
        "            earnings_calls.append({\"symbol\": symbol,\n",
        "                                  \"quarter\": resulting_json.get(\"quarter\"),\n",
        "                                  \"year\": resulting_json.get(\"year\"),\n",
        "                                  \"speaker_desig\":name_designation_lookup.get(transcript.get(\"name\"),\"unknown\"), \n",
        "                                  \"speaker_name\":transcript.get(\"name\"),\n",
        "                                  \"speech\":transcript.get(\"speech\")})\n",
        "  return earnings_calls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlVTIoaTpJRV"
      },
      "source": [
        "earnings_call_df = pd.DataFrame(create_earnings_calls_list(symbols))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "sBSjTr3ppi7S",
        "outputId": "6bf5bf80-f25c-41cc-843a-b71e8cd144a3"
      },
      "source": [
        "earnings_call_df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>quarter</th>\n",
              "      <th>year</th>\n",
              "      <th>speaker_desig</th>\n",
              "      <th>speaker_name</th>\n",
              "      <th>speech</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Director of Investor Relations and Corporate F...</td>\n",
              "      <td>Tejas Gala</td>\n",
              "      <td>[Thank you. Good afternoon and thank you for j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Chief Executive Officer</td>\n",
              "      <td>Tim Cook</td>\n",
              "      <td>[Thanks, Tejas. Good afternoon, everyone, and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>Luca Maestri</td>\n",
              "      <td>[Thank you, Tim. Good afternoon, everyone. We ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Director of Investor Relations and Corporate F...</td>\n",
              "      <td>Tejas Gala</td>\n",
              "      <td>[Thank you, Luca. [Operator Instructions] Oper...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Cross Research</td>\n",
              "      <td>Shannon Cross</td>\n",
              "      <td>[Tim, I had sort of a big picture question on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Chief Executive Officer</td>\n",
              "      <td>Tim Cook</td>\n",
              "      <td>[Sure, Shannon. We saw double-digit increases ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Cross Research</td>\n",
              "      <td>Shannon Cross</td>\n",
              "      <td>[Okay. And then, Luca, can you talk about gros...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>Luca Maestri</td>\n",
              "      <td>[Yes, Shannon. Yes, we did 42.5% during March,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Evercore</td>\n",
              "      <td>Amit Daryanani</td>\n",
              "      <td>[I have two as well. First one, just on servic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>Luca Maestri</td>\n",
              "      <td>[So Amit, the -- our Services business did bet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Evercore</td>\n",
              "      <td>Amit Daryanani</td>\n",
              "      <td>[Got it. That's helpful. And then, Tim, if I c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Chief Executive Officer</td>\n",
              "      <td>Tim Cook</td>\n",
              "      <td>[Yes. We're clearly seeing strong performance ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Morgan Stanley</td>\n",
              "      <td>Katy Huberty</td>\n",
              "      <td>[This was a pretty unbelievable quarter, and i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Chief Executive Officer</td>\n",
              "      <td>Tim Cook</td>\n",
              "      <td>[If you sort of look at the different products...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Morgan Stanley</td>\n",
              "      <td>Katy Huberty</td>\n",
              "      <td>[And then, Luca, as I look at inventory plus v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>Luca Maestri</td>\n",
              "      <td>[No. On -- as you think about the June quarter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Bank of America</td>\n",
              "      <td>Wamsi Mohan</td>\n",
              "      <td>[Tim, your content offerings are still at very...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Chief Executive Officer</td>\n",
              "      <td>Tim Cook</td>\n",
              "      <td>[TV+ -- let me start with TV+. TV+ is going ve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Bank of America</td>\n",
              "      <td>Wamsi Mohan</td>\n",
              "      <td>[Okay. As my follow-up, Luca, on the June quar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>2021</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>Luca Maestri</td>\n",
              "      <td>[So when you look at our normal seasonality, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   symbol  ...                                             speech\n",
              "0    AAPL  ...  [Thank you. Good afternoon and thank you for j...\n",
              "1    AAPL  ...  [Thanks, Tejas. Good afternoon, everyone, and ...\n",
              "2    AAPL  ...  [Thank you, Tim. Good afternoon, everyone. We ...\n",
              "3    AAPL  ...  [Thank you, Luca. [Operator Instructions] Oper...\n",
              "4    AAPL  ...  [Tim, I had sort of a big picture question on ...\n",
              "5    AAPL  ...  [Sure, Shannon. We saw double-digit increases ...\n",
              "6    AAPL  ...  [Okay. And then, Luca, can you talk about gros...\n",
              "7    AAPL  ...  [Yes, Shannon. Yes, we did 42.5% during March,...\n",
              "8    AAPL  ...  [I have two as well. First one, just on servic...\n",
              "9    AAPL  ...  [So Amit, the -- our Services business did bet...\n",
              "10   AAPL  ...  [Got it. That's helpful. And then, Tim, if I c...\n",
              "11   AAPL  ...  [Yes. We're clearly seeing strong performance ...\n",
              "12   AAPL  ...  [This was a pretty unbelievable quarter, and i...\n",
              "13   AAPL  ...  [If you sort of look at the different products...\n",
              "14   AAPL  ...  [And then, Luca, as I look at inventory plus v...\n",
              "15   AAPL  ...  [No. On -- as you think about the June quarter...\n",
              "16   AAPL  ...  [Tim, your content offerings are still at very...\n",
              "17   AAPL  ...  [TV+ -- let me start with TV+. TV+ is going ve...\n",
              "18   AAPL  ...  [Okay. As my follow-up, Luca, on the June quar...\n",
              "19   AAPL  ...  [So when you look at our normal seasonality, a...\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPL5Qu9ofc7w"
      },
      "source": [
        "Join the field speech, which is a list that I believe is always of lenth 1, and make it a string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpCBOANMeGpf"
      },
      "source": [
        "earnings_call_df['speech'] = earnings_call_df.speech.str.join('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR-pSAAviTyo"
      },
      "source": [
        "Not used but code here works on one ticker, AAPL, just to document the process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MC2r2iqiZnl",
        "outputId": "51037de2-f7a8-4dab-dd2d-9f90c3c06e1c"
      },
      "source": [
        "print(\"length of first speech:\", len(temp_df['speech2'][0]))\n",
        "print(\"length of entire dataframe:\", len(earnings_call_df))\n",
        "print(\"Unique symbols:\", earnings_call_df.symbol.unique())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJzzWSjTbPXt",
        "outputId": "82e7f63c-e6cc-4c9e-8d2f-d7225c4ba51e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG2dZYxBbUQe",
        "outputId": "75a4dcba-69c4-441c-f7d4-5b6b33331a92"
      },
      "source": [
        "earnings_call_df.symbol.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AAPL', 'MSFT', 'XOM', 'CVX'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpCgLG0bfCC"
      },
      "source": [
        "aapl_one_df = earnings_call_df[(earnings_call_df.symbol=='AAPL') & (earnings_call_df.year==2014) & (earnings_call_df.quarter==2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29_N1yvLbvNf",
        "outputId": "e413a837-871c-44cf-93d3-5472a9ed48a3"
      },
      "source": [
        "len(aapl_one_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu-1mTPMcXlG",
        "outputId": "41693d3f-7e76-4e68-8046-bafa6309928b"
      },
      "source": [
        "len(aapl_one_df.iloc[0,5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC0Ch5RncDw3",
        "outputId": "04e7cb99-9695-49e8-cb02-b2ec38b923d4"
      },
      "source": [
        "temp_count = 0\n",
        "for row in range(len(aapl_one_df)):\n",
        "    temp_count += len(aapl_one_df.iloc[row,5])\n",
        "print(temp_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op172gSYdt4B",
        "outputId": "807804be-1515-416b-b997-de1ab12cd968"
      },
      "source": [
        "max(earnings_call_df.speech.apply(len))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64462"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQXbhZYHdMVb"
      },
      "source": [
        "aapl_one_merged_df = aapl_one_df[['symbol','quarter', 'year', 'speech']].copy()\n",
        "aapl_one_merged_df['speech'] = aapl_one_merged_df.groupby(['symbol','quarter', 'year'])['speech'].transform(lambda x: ' '.join(x))\n",
        "aapl_one_merged_df = aapl_one_merged_df[['symbol','quarter', 'year', 'speech']].drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS8oPC7Lg1m2",
        "outputId": "e7be4329-79ed-4871-de24-ba59e5659dab"
      },
      "source": [
        "len(aapl_one_merged_df.speech.iloc[0])\n",
        "#aapl_one_merged_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47055"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTpyzvTA27jU"
      },
      "source": [
        "# Sentence Relevance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWLgkMauLtZK"
      },
      "source": [
        "compare_sentences = [\"In order to achieve these stated goals, , with the need to transition to a net zero carbon economy to stabilize global temperatures.\",\n",
        "                     \"Broader societal role and considering value creation and erosion across multiple capitals: natural, social, human, financial, intellectual and manufactured capital, hunger and access to healthcare and education, equitable world\",\n",
        "                     \"I look forward to working with peers from organizations that span multiple sectors and geographies. Working together at A4S, I believe we can work together, share best practices and come up with practical solutions that allow us to move our organizations – and encourage others – toward more sustainable outcomes.\"\n",
        "                     ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLCc24HXOCmv"
      },
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQasXKuQRZny"
      },
      "source": [
        "def sort_sents_by_desc_order_relevance(earnings_call_df, compare_sentences):\n",
        "  sorted_sents_for_speech = []\n",
        "  sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "  for speech_index, speech in earnings_call_df.speech.items():\n",
        "    sentences = nltk.sent_tokenize(speech[0])\n",
        "    \n",
        "    tuples_of_sents_sim = [(sent, \n",
        "                        sum([cosine(sbert_model.encode([compare_sent])[0], \n",
        "                                                sbert_model.encode([sent])[0])for compare_sent in compare_sentences])/len(compare_sentences))\n",
        "                                                for sent in sentences]\n",
        "    tuples_of_sents_sim.sort(key=lambda x:x[1], reverse=True)\n",
        "    sorted_sents_for_speech.append(\" \".join([x[0] for x in tuples_of_sents_sim]))\n",
        "  return sorted_sents_for_speech"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo9N43BPwlWH"
      },
      "source": [
        "limited_earnings_call_df = earnings_call_df[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmd5O8_2zLqo",
        "outputId": "467233d2-b9fe-45c8-8096-7d619e9c1a3f"
      },
      "source": [
        "sort_sents_by_desc_order_relevance(limited_earnings_call_df, compare_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Please note that some of the information you'll hear during our discussion today will consist of forward-looking statements, including, without limitation, those regarding revenue, gross margin, operating expenses, other income and expense, taxes, capital allocation and future business outlook, including the potential impact of COVID-19 on the Company's business results of operation. These statements involve risks and uncertainties that may cause actual results or trends to differ materially from our forecast. After that, we'll open the call to questions from analysts. I'd now like to turn over the call to Tim for introductory remarks. For more information, please refer to the risk factors discussed in Apple's most recently filed annual report on Form 10-K and the Form 8-K filed with the SEC today, along with the associated press release. Speaking first today is Apple's CEO, Tim Cook, and he'll be followed by CFO, Luca Maestri. Good afternoon and thank you for joining us. Thank you. Apple assumes no obligation to update any forward-looking statements or information which speak as of their respective dates.\",\n",
              " \"We're also keenly focused on how this wave of green innovation can lead to equitably shared prosperity. In addition to the progress we've made in our own efforts to achieve our pledge of a net zero carbon footprint by 2030 across our entire supply chain and use of our products, we're proud to play a role in the growing ripple change taking place across the private sector. The investments will support American innovation and drive economic benefits in every state, including a new North Carolina campus and job creating investments in innovative fields like silicon engineering and 5G technology. We also want to do everything we can to connect users to life-saving vaccinations that are in ever greater supply. As we look forward to WWDC, we're taking new steps to support and foster the unmatched community of developers we work with here in the United States and around the world. Across our products and throughout our software ecosystem, we continue to deploy industry-leading new tools to protect users' fundamental right to privacy. We continue to enhance and improve our current service offerings from Apple Music to Apple News while continuing to launch new services that enhance our customers' lives. And here in the United States, we started a green impact accelerator, investing and supporting minority-owned businesses at the forefront of environmental fields. Reflecting both the enduring ways our products have helped our users meet this moment in their own lives as well as the optimism consumers seem to feel about better days ahead, we set new March quarter records in every geographic segment, and success was broadly distributed across our product categories. While we're on the topic of services, in many ways, this quarter showed the unique value to customers created by Apple's belief in the deep integration of hardware, software and services. We were proud to announce that we have expanded and accelerated our commitment to the U.S. economy. Beginning with an enduring and uncompromising commitment to the health and safety of our teams, and extending well beyond our walls into the communities where we work. In addition to the App Store privacy nutrition labels that we discussed on last quarter's call, we're proud to have launched the full implementation of App Tracking Transparency. To provide some color on our results, let's turn to our product categories. AirTag builds on the powerful and incredibly useful Find My experience, helping users privately and securely keep track of the items that matter most to them. Third-party accessories and products can also make use of the Find My network guaranteeing a great experience no matter what products you choose to use. As has been the case throughout the pandemic, iPad and Mac continue to be critically important tools for our customers. Apple TV+ also continues to be a place where we can tell stories that matter and lift up important voices and experiences like our new upcoming content partnership with Malala, our latest original documentary special, the Year the Earth changed narrated by the legendary David Attenborough and released to commemorate Earth Day. Building on the success of our entrepreneur camp program, which we began in 2019, this program gives this profoundly innovative community of developers the chance to develop next-level technical skills through hands-on technology labs, and with our partners at Harlan Capital, it also shares insights and mentorship on building and scaling an app business. We also announced Apple Podcast subscriptions, a global marketplace for listeners to discover premium content from their favorite creators and storytellers. This powerful yet simple idea gives users a choice over how their data is used and shared across the apps that they love and use every day. Through Apple's $4.7 billion in green bonds and related efforts, we've supported transformative environmental projects around the world from clean energy initiatives in China to two of the world's largest onshore wind turbines in Denmark to 180-acre solar project outside Reno, Nevada and many more. Through our new $200 million restore fund, we're helping local and rural communities around the world, build sustainable industries around working for us creating opportunities and removing up to 1 million metric tons of carbon from the atmosphere every year. It's worth remembering for much more than financial reasons or year-ago compares, just how we felt at this time last year when everything we knew had to change. Instead of simply assuming that the end is in sight, we at Apple are doing our part to make it a reality. Looking forward, whether you're running a business or just hoping to see family again after more than a year, it's tempting at this moment to let hope about the end of the COVID-19 pandemic outstripped clear-eyed realism about the challenges we still face. I'm particularly excited about our inaugural entrepreneur camp for black to founders and developers. and to enhance remote work in all of its forms. Through Apple Maps, for example, we now showcase vaccine site locations here in the United States, building on our maps of testing locations in many countries around the world. This is, of course, just one example of how Apple lives its values and operationalizes the idea that to whom much is given, much as expected. To begin with our environmental efforts, just last week, we marked a milestone Earth Day on multiple fronts. Apple Watch is a global success story, and the category set March quarter records in each geographic segment, thanks to strong performance from both Apple Watch Series 6 and Apple Watch SE. It's an exciting and busy period ahead for Wearables, Home and Accessories with the launch of the next-generation Apple TV 4K and our newest accessory, AirTag. On Mac, fueled by the M1, we set an all-time revenue record continuing the momentum for the product category. With that, I'll hand things over to Luca. We have reached new days of hopeful resolve. We debuted a radically redesigned brand-new iMac designed around M1's unmatched capabilities, and we've brought M1 to iPad for the first time in the new iPad Pro with 5G capability and a Liquid Retina XDR display. Ted Lasso, in particular, has been recognized with a multitude of awards and nominations including most recently, an AFI Program of the Year Recognition, Writers Guild of America Awards and a clean sweep at the Critics Choice Awards. Just last week, we introduced Apple Card family which reinvents how you can share credit cards and build credit together. Good afternoon, everyone, and thanks for joining the call today. In many places around the world, new waves of infections driven by even more infectious variants of the virus are driving new lockdowns. With unmatched 5G capability, the best camera system ever in an iPhone and advanced durability from ceramic shield, this family of devices is popular with both upgraders and new customers alike. No matter what device you enjoy it from, it is a milestone period for Apple TV+, racking up many new award nominations and wins, including its first Oscar nominations. Our work is not done, but as I said a year ago, while we can't say for sure how many chapters are in this book, we can have confidence that the ending will be a good one. Last week, both iPad and Mac took a big step forward. Mac and Services delivered all-time record results, and we set new March quarter records for iPhone and Wearables, Home and Accessories. As of this month, 110 of our suppliers have joined us in our renewable energy commitment, and we will bring online nearly 8 gigawatts of new clean energy, the equivalent of taking 3.4 million gas-powered vehicles off the road each year. Turning to Services. Thanks to researchers and scientists, doctors and nurses, everyone who can put a shot in an arm and even just check a name off a list. Apple is proud to report another strong quarter, one where we set new March quarter records for both revenue and earnings, besting our year ago revenue performance by 54%. Over the past year, tens of millions of iPads and Macs have been deployed to help students learn, creators create. We achieved growth of 27% year-over-year and set new records for services in each of our geographic segments. Thanks, Tejas. This has helped iPad grow very strong double digits to its highest March quarter revenue in nearly a decade. And just last week, we unveiled an all-new purple finish for iPhone 12 and 12 Mini. We saw a very strong performance for iPhone, which grew 66% year-over-year driven by the strong popularity of the iPhone 12 family. It was a quarter of sustained strength for Wearables, Home and Accessories, which grew by 25% year-over-year. Over the next five years, we will invest $430 billion, creating 20,000 jobs in the process. In fact, the last three quarters for Mac have been its three best quarters ever. People left groceries or care packages sitting in the garage or in the hall overnight in recognition of all that we didn't know and therefore, had to imagine. Plains set grounded, entire business districts were empty and silent.\",\n",
              " \"Finally, we're adding new services that we think our customers will love, while also continuing to improve the breadth and quality of our current service offerings. We continue to believe there is great value in our stock and maintain our target of reaching a net cash neutral position over time. This strong position allows us to continue to invest confidently in our future, while also returning value to our shareholders. We're very excited about the future of this category and believe that our integration of hardware, software and services uniquely positions us to provide great customer experiences in this category. The key drivers for our services business all continue to move in the right direction. We are extremely pleased to report record results for our March quarter despite continued uncertainty in the macro environment. In the enterprise market, customers across many industries are accelerating their adoption of iPhone 12 and 5G as a key platform for the future of their business. I will provide more details about the performance of our services business later. Our new service offerings, Apple TV+, Apple Arcade, Apple News Fitness+ as well as the Apple One bundle, continue to scale across users, content and features and are contributing to overall services growth. Both Mac and iPad are incredibly relevant products for our customers in the current working and learning environments, and we are delighted that the most recent surveys of U.S. consumers from 451 Research measured customer satisfaction at 91% for Mac and 94% for iPad. Let me get into more detail for each of our revenue categories. Given the confidence we have in our business today and into the future, our Board has authorized an additional $90 billion for share repurchases. Given the continued uncertainty around the world in the near term, we are not providing revenue guidance, but we are sharing some directional insights assuming that the COVID-related impacts to our business do not worsen from what we are seeing today for the current quarter. This has allowed their staff to rapidly scan and register new patients and vastly increase their daily vaccination capacity. As we move ahead into the June quarter, I'd like to review our outlook, which includes the types of forward-looking information that Tejas referred to at the beginning of the call. As we continue to execute at an extremely high level, we were also able to return nearly $23 billion to shareholders during the March quarter. Products gross margin was 36.1%, growing 100 basis points sequentially also thanks to cost savings and FX, partially offset by seasonal loss of leverage. However, we believe that the sequential revenue decline from the March quarter to the June quarter will be greater than in prior years for two reasons. Delta Airlines, for example, is putting iPhone 12 and 5G connectivity into the hands of flight attendants so they can provide the best passenger service possible as air travel rebounds. This amazing performance was driven by the very enthusiastic customer response to our new Macs powered by the M1 chip. As a result of this level of sales performance and the unmatched loyalty of our customers, our installed base of active devices reached a new all-time record in each of our major product categories. First, our installed base growth has accelerated and reach an all-time high across each major product category. Thanks to the exceptional loyalty of our customer base and strength of our ecosystem, our active installed base of iPhones reached a new all-time high. We established new records in each geographic segment and in most service categories. For example, Apple Arcade launched its biggest expansion yet adding incredibly fun games to the catalog, including new exclusive Arcade originals, along with two entirely new categories, App Store greats and timeless classics. We are innovating and investing at an unprecedented pace, including accelerating our investment in the United States with our new commitment to contribute more than $430 billion and 20,000 jobs to the country over the next five years. Openreach in the U.K. has started equipping tens of thousands of field engineers with iPhone 12 to speed up their deployment of broadband services to homes around the country. Next, I'd like to talk about Mac. With this level of customer satisfaction, and with around half of the customers purchasing Mac and iPad during the quarter being new to that product, the active installed base for both products continues to grow nicely and reached new all-time highs. Third, paid subscriptions continued to show strong growth. First, keep in mind that due to the later launch timing and strong demand, iPhone only achieved supply-demand balance during the March quarter. Apple Pay continues to expand geographically, launching in Mexico and in South Africa, bringing our payment service to six continents. We grew very strongly in every geographic segment with an all-time record in Japan and a March quarter record in rest of Asia Pacific. We're also raising our dividend by 7% to $0.22 per share, and we continue to plan for annual increases in the dividend going forward. With that, let's open the call to questions. Performance was consistently strong across the world as we grew strong double digits in each geographic segment and set March quarter records in most markets we track. We've been operating in new ways for over a year, and we could not be more proud of the way our team continues to execute and innovate at unprecedented levels. Second, we believe supply constraints will have a revenue impact of $3 billion to $4 billion in the June quarter. We also set new March quarter records in every geographic segment with growth of at least 35% in each one of them. Company gross margin was 42.5%, up 270 basis points from last quarter driven by cost savings, a strong mix and favorable foreign exchange. We expect OpEx to be between $11.1 billion and $11.3 billion. We grew very strong double digits in each of our product categories, with all-time records for Mac and for Services and March quarter records for iPhone and for Wearables, Home and Accessories. Second, the number of both transacting and paid accounts on our digital content stores reached a new all-time high during the March quarter, with paid accounts increasing double digits in each of our geographic segments. This will cause a steeper sequential decline than usual. Let me now turn to our cash position. Turning to Services. Net income of $23.6 billion, diluted earnings per share of $1.40 and operating cash flow of $24 billion were all March quarter records by a wide margin. We expect our June quarter revenue to grow strong double digits year-over-year. We reached an all-time revenue record of $16.9 billion with all-time records for the App Store, cloud services, music, video, advertising and payment services. And UCHealth, a large health care provider in Colorado, was able to reduce per patient vaccination time from 3 minutes to only 30 seconds largely by moving from PC stations to iPhones. Thank you, Tim. We issued $14 billion of new term debt and retired $3.5 billion of term debt leaving us with total debt of almost $122 billion. We set an all-time revenue record of $9.1 billion, up 70% over last year, and grew very strongly in each geographic segment with all-time revenue records in Europe and rest of Asia Pacific and March quarter records in the Americas, Greater China and Japan. iPad performance was also outstanding with revenue of $7.8 billion up 79%. In the U.S. the latest survey of consumers from 451 Research indicates customer satisfaction of over 99% for the iPhone 12 family. During the March quarter, we added more than 40 million paid subs sequentially, and we have now reached more than 660 million paid subscriptions across the services on our platform. Good afternoon, everyone. Finally, reflecting the approved 7% dividend increase I just mentioned, today, our Board of Directors has declared a cash dividend of $0.22 per share of common stock payable on May 13, 2021, to shareholders of record as of May 10, 2021. Apple Watch continues to extend its reach, with nearly 75% of the customers purchasing Apple Watch during the quarter being new to the product. This included $3.4 billion in dividends and equivalents and $19 billion through open market repurchases of 147 million Apple shares. Our revenue reached a March quarter record of $89.6 billion, an increase of over $31 billion or 54% from a year ago. Our services set an all-time record of $16.9 billion, growing 27% over a year ago. We expect OI&E to be around $50 million and our tax rate to be around 14.5%. Services gross margin was 70.1%, up 170 basis points sequentially and mainly due to a different mix. iPhone revenues had a March quarter record of $47.9 billion, growing 66% year-over-year as the iPhone 12 family continue to be in high demand. Wearables, Home and Accessories grew 25% year-over-year to $7.8 billion, setting new March quarter revenue records in every geographic segment. We expect gross margin to be between 41.5% and 42.5%. We ended the quarter with over $204 billion in cash plus marketable securities. As a result, net cash was $83 billion at the end of the quarter. Products revenue was a March quarter record of $72.7 billion, up 62% over a year ago. This is up $145 million from just a year ago and twice the number of paid subscriptions we are only 2.5 years ago.\",\n",
              " '[Operator Instructions] Operator, may we have the first question, please? Thank you, Luca.',\n",
              " \"How are you thinking about the opportunity for refreshing the installed base and attracting new customers? And are you seeing lives shorten given some of the programs that are being put out there by the carriers and by yourself? Just kind of maybe big picture, if you can talk about what you're seeing in terms of iPhone out there in the market. I'm just curious, there are so many different things happening in this cycle, 5G, pandemic. Tim, I had sort of a big picture question on iPhone.\",\n",
              " \"And so a lot of the 5G upgrades will be in front of us, not behind us. And so we like what we see. They're moving quickly in the United States. So -- and in fact, in the March quarter, there was actually a record number of upgraders for a March quarter. But penetration is still -- on a global level, is still low at this point. We saw double-digit increases on a year-over-year basis on both the new to iPhone and upgraders. You see in China, things have moved quickly to 5G. Different countries are in different points. But a lot of the other regions are slower to adopt and slower to gain coverage in 5G. It's obviously the early days of 5G. Sure, Shannon.\",\n",
              " \"And then, Luca, can you talk about gross margin? I mean 42% is higher than it's been that I can kind of remember actually at this point, so maybe if you talk about the drivers of gross margin, and maybe if there were any offsets from higher component costs or the logistics costs that obviously were overshadowed by currency and other things? Okay.\",\n",
              " \"Cost savings, which has been good for us during the cycle. As we transition into June, as you know, that we will expect some level of deleverage but that will be offset by cost savings. A really strong mix, a strong mix on iPhone, but in general, across all product categories, and that obviously was helpful. And foreign exchange sequentially, again, from December to March, was favorable 90 basis points. So that helped as well. So for March, we were up 270 basis points sequentially, really driven by three major factors. So those are the three major factors there. Yes, we did 42.5% during March, and we've guided to similar, slightly lower levels for June. Foreign exchange doesn't have much of an impact as we go from March to June. Yes, Shannon.\",\n",
              " \"I'd like to just understand what do you think drove that acceleration specifically? It turned out it actually accelerated for us. And is mid-20% sort of the growth norm as we go forward for Services? First one, just on services, I think 90 days ago, the expectation was that line item would decelerate a little bit into the March quarter. I have two as well.\",\n",
              " \"And so when we look at all these fundamental vectors of our Services business, obviously, we feel very good. And advertising, obviously, consumer sentiment has improved and advertising is coming back. Are we improving the quality of the existing services? But in general, I talked during our prepared remarks, I mean, there are a number of things that we always look at around the Services business, how many new paid accounts do we have, what number of new subscriptions do we get that -- above all, is our installed base continuing to grow. And so the combination of these factors really delivered this very, very strong performance during the March quarter. One of the things that we've noticed is that throughout COVID was that obviously digital services have done very well. It was stronger across the board. During the March quarter, we've seen a return to growth on Apple Care and obviously, we've reopened a lot of the stores during the course of the quarter. Are we adding new services? So Amit, the -- our Services business did better than what we were expecting when we had the last call in January. We feel very good about it. And then we've had a couple of categories like Apple Care because many of the points of sale and stores were closed and advertising because of the reduced economic activity that were negatively affected during COVID. As we look ahead, as you know, we don't provide specific guidance for our product categories.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "w4m3zJSsp5hU",
        "outputId": "7d4cbf12-d821-45a8-e676-33f4aa26d5c6"
      },
      "source": [
        "limited_earnings_call_df[\"relevanceOrderedSents\"] = sort_sents_by_desc_order_relevance(limited_earnings_call_df, compare_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b30cd73098ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlimited_earnings_call_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"relevanceOrderedSents\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_sents_by_desc_order_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimited_earnings_call_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sort_sents_by_desc_order_relevance' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "3xbuoIDTqAPg",
        "outputId": "a36a9daf-209a-426c-d01c-6d1f192f4b06"
      },
      "source": [
        "limited_earnings_call_df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7ced2a84cfff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlimited_earnings_call_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'limited_earnings_call_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-gkWfj10XdW"
      },
      "source": [
        "# Sustainaibility Reports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9QT5koGU1yR"
      },
      "source": [
        "#ADF Reports\n",
        "adfreports = []"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D27W1Oursc49"
      },
      "source": [
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsUhL3rSWPO5"
      },
      "source": [
        "# local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/data/a4s')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph0eb1oogeIE"
      },
      "source": [
        "# 2. Auto-iterate using the query syntax\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1wjLj01vGtA6zDxulEU9pxH9_dZcGkTl5' in parents\"}).GetList()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMTOZG5xogsJ",
        "outputId": "e3f5bdad-66a3-426d-87c6-f34928ebbc08"
      },
      "source": [
        "print(file_list[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GoogleDriveFile({'kind': 'drive#file', 'id': '1XD3afMWpx_HWqATn6mJuPOUl2wdV65Lc', 'etag': '\"MTYyNDkyNjYxMzAwMA\"', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1XD3afMWpx_HWqATn6mJuPOUl2wdV65Lc', 'webContentLink': 'https://drive.google.com/uc?id=1XD3afMWpx_HWqATn6mJuPOUl2wdV65Lc&export=download', 'alternateLink': 'https://drive.google.com/file/d/1XD3afMWpx_HWqATn6mJuPOUl2wdV65Lc/view?usp=drivesdk', 'embedLink': 'https://drive.google.com/file/d/1XD3afMWpx_HWqATn6mJuPOUl2wdV65Lc/preview?usp=drivesdk', 'iconLink': 'https://drive-thirdparty.googleusercontent.com/16/type/application/pdf', 'thumbnailLink': 'https://lh3.googleusercontent.com/CDJd_feJeCUMyMrGvT7ECGazrQ4TDeqAP5bAMxr0Vehqe4aYSyQl1HisAYrYQ9ZA1wK_5Io0FNzR6Zk=s220', 'title': '12567_FFTF Rabobank case study v1.pdf.downloadasset.pdf', 'mimeType': 'application/pdf', 'labels': {'starred': False, 'hidden': False, 'trashed': False, 'restricted': False, 'viewed': True}, 'copyRequiresWriterPermission': False, 'createdDate': '2021-06-29T02:04:54.133Z', 'modifiedDate': '2021-06-29T00:30:13.000Z', 'modifiedByMeDate': '2021-06-29T00:30:13.000Z', 'lastViewedByMeDate': '2021-06-29T02:04:54.133Z', 'markedViewedByMeDate': '1970-01-01T00:00:00.000Z', 'version': '4', 'parents': [{'kind': 'drive#parentReference', 'id': '1wjLj01vGtA6zDxulEU9pxH9_dZcGkTl5', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1XD3afMWpx_HWqATn6mJuPOUl2wdV65Lc/parents/1wjLj01vGtA6zDxulEU9pxH9_dZcGkTl5', 'parentLink': 'https://www.googleapis.com/drive/v2/files/1wjLj01vGtA6zDxulEU9pxH9_dZcGkTl5', 'isRoot': False}], 'downloadUrl': 'https://www.googleapis.com/drive/v2/files/1XD3afMWpx_HWqATn6mJuPOUl2wdV65Lc?alt=media&source=downloadUrl', 'userPermission': {'kind': 'drive#permission', 'etag': '\"BXUZgjjboejcGByAIOmOI1pt9kc\"', 'id': 'me', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1XD3afMWpx_HWqATn6mJuPOUl2wdV65Lc/permissions/me', 'role': 'owner', 'type': 'user'}, 'originalFilename': '12567_FFTF Rabobank case study v1.pdf.downloadasset.pdf', 'fileExtension': 'pdf', 'md5Checksum': '99ddbfeacdb0e4e61fdfca0b1982f20e', 'fileSize': '2126299', 'quotaBytesUsed': '2126299', 'ownerNames': ['Srishti Mehra'], 'owners': [{'kind': 'drive#user', 'displayName': 'Srishti Mehra', 'picture': {'url': 'https://lh3.googleusercontent.com/a-/AOh14GiVF9wdDD41n0Vuwmk75OZTpSlFvihpG-_E44nS=s64'}, 'isAuthenticatedUser': True, 'permissionId': '05008182001513680726', 'emailAddress': 'srishtimehra@berkeley.edu'}], 'lastModifyingUserName': 'Srishti Mehra', 'lastModifyingUser': {'kind': 'drive#user', 'displayName': 'Srishti Mehra', 'picture': {'url': 'https://lh3.googleusercontent.com/a-/AOh14GiVF9wdDD41n0Vuwmk75OZTpSlFvihpG-_E44nS=s64'}, 'isAuthenticatedUser': True, 'permissionId': '05008182001513680726', 'emailAddress': 'srishtimehra@berkeley.edu'}, 'capabilities': {'canCopy': True, 'canEdit': True}, 'editable': True, 'copyable': True, 'writersCanShare': True, 'shared': True, 'explicitlyTrashed': False, 'appDataContents': False, 'headRevisionId': '0B4PYReHrhYYkMjZpVVZGcnlHMDFGZDVkZlh5aTVXYkxLU3RzPQ', 'spaces': ['drive']})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcN_MGLkg54j",
        "outputId": "190891ef-9135-4a80-9dd7-12225ea44801"
      },
      "source": [
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)\n",
        "  \n",
        "  # creating a pdf file object\n",
        "  pdfFileObj = open(fname, 'rb')\n",
        "\n",
        "  # creating a pdf reader object\n",
        "  pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "  \n",
        "  # printing number of pages in pdf file\n",
        "  print(pdfReader.numPages)\n",
        "  \n",
        "  # creating a page object\n",
        "  pageObj = pdfReader.getPage(0)\n",
        "  \n",
        "  # extracting text from page\n",
        "  text = [pdfReader.getPage(i).extractText() for i in range(0, pdfReader.getNumPages())]\n",
        "  adfreports.append({\"resource_name\":f['title'], \"text\":\"\\n\".join(text)})\n",
        "  \n",
        "  # closing the pdf file object\n",
        "  pdfFileObj.close()\n",
        "\n",
        "  #print('downloading to {}'.format(fname))\n",
        "  #f_ = drive.CreateFile({'id': f['id']})\n",
        "  #f_.GetContentFile(fname)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: 12567_FFTF Rabobank case study v1.pdf.downloadasset.pdf, id: 1XD3afMWpx_HWqATn6mJuPOUl2wdV65Lc\n",
            "downloading to /root/data/a4s/12567_FFTF Rabobank case study v1.pdf.downloadasset.pdf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "title: A4S Essential Guide to Valuations and Climate Change.pdf.downloadasset.pdf, id: 1d68YLejIXpt5MdbdyQLyz8r4_HEeS6l_\n",
            "downloading to /root/data/a4s/A4S Essential Guide to Valuations and Climate Change.pdf.downloadasset.pdf\n",
            "104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHiuLFmxhAXw"
      },
      "source": [
        "adfreportsdf = pd.DataFrame(adfreports)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvVkvucvkqWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2cdc6d9b-5107-4f3d-c327-abc1179a519e"
      },
      "source": [
        "adfreportsdf"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resource_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12567_FFTF Rabobank case study v1.pdf.download...</td>\n",
              "      <td>www.˜nanceforthefuture.org\\nIn partnership wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A4S Essential Guide to Valuations and Climate ...</td>\n",
              "      <td>ESSENTIAL GUIDE TO  \\nVALUATIONS AND CLIMATE \\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       resource_name                                               text\n",
              "0  12567_FFTF Rabobank case study v1.pdf.download...  www.˜nanceforthefuture.org\\nIn partnership wit...\n",
              "1  A4S Essential Guide to Valuations and Climate ...  ESSENTIAL GUIDE TO  \\nVALUATIONS AND CLIMATE \\..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EtzgRGhGAhVo",
        "outputId": "bd9b2629-a244-4c56-ef74-4d95808f8c10"
      },
      "source": [
        "adfreportsdf.text[1]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ESSENTIAL GUIDE TO  \\nVALUATIONS AND CLIMATE \\n\\nCHANGE\\nA framework to assess the impact of climate change \\non business valuations\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\nIntroduction\\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\n\\nA4S Essential Guide to Valuations and Climate Change \\n         \\n \\n \\n \\n \\n    2Introduction\\nAutorité et champ \\nd™application du cadre\\nCadre d™évaluation axé \\nsur les changements \\n\\nclimatiques\\nAnnexes 1-4\\n\\nAnnexe 5 Œ Études de cas\\nGUIDE ESSENTIEL SUR \\nLES ÉVALUATIONS ET LES \\n\\nCHANGEMENTS CLIMATIQUES\\nRÉSEAU\\n DE LEADERSHIP \\nDES CHEFS DES \\nFINANCES DE \\nL™ADC\\nIntroduction\\nAuthority and scope of \\n \\nthe framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\nNAVIGATING THIS GUIDE\\nWHAT IS INCLUDED\\n\\n\\nŁForeword from David McGraw,  \\n\\nCFO, OTPP\\nŁExecutive summary\\nŁWhy did we develop this \\n\\nframework?\\nŁWhat is climate change?\\nŁWhat is the link between \\n\\nclimate ch\\nange and business \\nvaluation?\\nŁCurrent approach to climate \\n\\nchange in v\\naluations\\nSECTION 1: INTRODUCTION\\nŁWho are the key stakeholders?\\nŁ\\n\\nlimitations\\nSECTION 2: AUTHORITY AND\\n SCOPE OF THE FRAMEWORK\\nSECTION 3: CLIMATE CHANGE \\n VALUATION FRAMEWORK\\nŁOverview\\nŁIdentify\\nŁAssess\\nŁFilter\\nŁIntegrate\\nŁTriangulate\\n1.Further detail on climate \\nchange factors and their \\n\\nimpact on valuations\\n2.\\n External data providers\\n3.\\n\\n\\nreporting\\n4.\\n Metrics recommended in the \\n\\nTCFD guidance and possible \\n\\nsources\\n5.\\nCase studies\\nAPPENDICES\\n\\nA4S Essential Guide to Valuations and Climate Change \\n3Introduction\\nAutorité et champ \\nd™application du cadre\\nCadre d™évaluation axé \\nsur les changements \\n\\nclimatiques\\nAnnexes 1-4\\n\\nAnnexe 5 Œ Études de cas\\nGUIDE ESSENTIEL SUR \\nLES ÉVALUATIONS ET LES \\n\\nCHANGEMENTS CLIMATIQUES\\nRÉSEAU\\n DE LEADERSHIP \\nDES CHEFS DES \\nFINANCES DE \\nL™ADC\\nIntroduction\\nAuthority and scope of \\n \\nthe framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\nTHE PROJECT TEAM\\nWe would like to \\nthank all of the project \\n\\nteam members who \\n\\ncontributed to the \\nframework.\\nA4S CFO LEADERSHIP NETWORK \\nPROJECT TEAM\\nDavid McGraw\\n\\n\\nLaura Clark\\n\\nSandra McEwen,\\n\\n\\nPension PlanJohn Le\\nvi\\n\\n\\n\\nDebor\\nah Ng\\n\\n\\n\\nTobin S\\nhields\\n\\n\\n\\nDoug Be\\nll\\nKatie Beith\\n\\nBarbara Salazer,\\n\\n\\nJessica Gilbert\\n\\n\\nKevin Hutchinson\\n\\nValuations, OMERS\\n\\nDavid K\\nrant\\n\\nManagement\\nThi Minh Tran\\n\\n\\nÉmilie Desloges\\n\\nKasia Pankowski\\n\\n\\nAudrey St-Jean\\n\\n\\n\\nCatalin\\na Miranda,\\n\\n\\n\\nSteph\\nanie Fox\\n\\n\\n\\nMary Ja\\nne Andrews\\n\\n\\nA4S AND CPA CANADA TEAM\\nJessica Fries\\nDavinder Valeri\\n\\nJamie Stewart\\n\\nAndrew Kornel\\n\\n\\n\\n\\nTilia Astell\\n\\n\\n   A4S Essential Guide to Valuations and Climate Change\\n4Title of guide to go here\\n4A4S Essential Guide to Valuations and Climate Change\\n4Ł Ł Ł Ł Ł Ł Introduction\\n\\nCFO, OTPP\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAuthority and scope of \\n the framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\nINTRODUCTION\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 5Title of guide to go here\\n 5A4S Essential Guide to Valuations and Climate Change\\n 5INTRODUCTION FROM THE A4S CFO LEADERSHIP NETWORK\\nClimate change presents a material risk to businesses and the \\neconomy. The impact of climate change is, therefore, an increasingly \\n\\nimportant consideration when making investment decisions and \\n\\ndetermining the value of businesses. For us at OTPP (Ontario \\n\\nTeachers™ Pension Plan), a substantial portion of our investments \\n\\nis in private assets, including direct investments in private equity, \\n\\ninfrastructure, natural resources, and real estate. And therefore, \\n\\nbusiness valuations are particularly important to us. We must \\n\\nbe able to assess accurately the value of each of these private \\n\\n\\n\\nour investment decisions. Climate change and business valuations \\n\\nare inextricably linked. When determining the value of a business, \\n\\none must consider all the risks and opportunities, of which climate \\n\\nchange is one. \\n\\nimpact of growing climate change risks in our valuations and did \\n\\na scan for guidance available from valuation organizations around \\n\\nthe globe and found that there was very little. This lack of guidance \\n\\nhighlighted the need for a valuation framework that would facilitate \\n\\na consistent approach to factoring in climate change risks into \\n\\nvaluation and investment analyses. \\nThis led me to approach A4S with the idea of creating some \\nguidance that investors, valuators and other interested parties could \\n\\nuse to help them include consideration of climate change issues \\n\\nin business valuations. It was on this basis that ﬁThe Impact of \\n\\nClimate Change on Business Valuationsﬂ working group was formed, \\n\\nbringing together a number of members of the CFO Leadership \\nNetwork in Canada as well as drawing on leading practice \\n\\nfrom elsewhere.  \\nFactoring climate change into valuations is in its infancy. Through \\nthis guide we hope to inform the discussion among the valuation, \\n\\naccounting and regulatory communities to encourage moving this \\n\\nforward into accepted practice. By doing so, we would expect more \\n\\nrobust disclosures in corporate reporting and a greater oversight by \\n\\nboards on this topic.  \\nWe hope that the guide will become widely referenced and used \\nby valuators and investors in their valuations and decision-making \\n\\nprocesses.\\nDAVID MCGRAW, \\n CHIEF FINANCIAL OFFICER, \\n ONTARIO TEACHERS™ \\n PENSION PLAN \\nﬁWhen determining the value \\nof a business, one must \\nconsider all the risks and \\nopportunities, of which climate \\nchange is one.\\nﬂIntroduction\\nŁ \\nCFO, OTPP\\nŁ \\nŁ \\n\\n\\nŁ \\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\nAuthority and scope of \\n the framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 6EXECUTIVE SUMMARY\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1. Identify\\n\\n2.\\n Assess\\n\\n3.\\n Filter\\n\\n\\n4.\\n Integrate\\n\\n5.\\n Triangulate\\n\\n\\n\\n\\n\\n\\n\\nﬁFactoring climate change into valuations is in its \\ninfancy. Through this guide we hope to inform the \\ndiscussion among the valuation, accounting and \\nregulatory communities to encourage moving this \\nforward into accepted practice. By doing so, we would \\nexpect more robust disclosures in corporate reporting \\nand a greater oversight by boards on this topic.ﬂ\\nDAVID MCGRAW, CHIEF FINANCIAL OFFICER, \\nONTARIO TEACHERS™ PENSION PLAN AND \\nMEMBER OF THE A4S CFO LEADERSHIP NETWORK\\nIntroduction\\nŁ \\nCFO, OTPP\\nŁ \\nŁ \\n\\n\\nŁ \\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\nAuthority and scope of \\n the framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nFRAMEWORK AIMS\\nThis framework has been developed to help \\ninvestors and valuators incorporate climate \\n\\nchange factors into business and asset \\n\\nvaluations. In recent years, climate change has \\n\\ncome to the forefront of broader environment, \\n\\nsocial and governance (ESG) considerations for \\n\\ncompanies.\\nThe goal of this framework is to:\\n\\nŁ Provide a reference point for investment \\nand valuation communities on valuation \\n\\nconsiderations as they relate to the \\n\\nincorporation of climate-related impacts.\\nŁ Act as a resource that facilitates discussion \\n\\non climate change which will enable \\n\\nstakeholders to gather the relevant \\n\\ninformation needed to assess climate change \\n\\nrisks and opportunities in their investment \\n\\nand valuation analyses.\\nŁ Provide stakeholders with a framework that \\n\\nwill enable them to consider systematically \\n\\nthe risks and opportunities associated with \\n\\nclimate change in investment decisions and \\n\\nfair value determinations. (These risks and \\n\\nopportunities should still be viewed under \\n\\ncore valuation principles similar to the ways \\n\\nin which other factors may be considered.)\\nTHE DRIVERS TO ACT\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWHY DID WE DEVELOP THIS \\nFRAMEWORK?\\n\\n\\n\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 7Introduction\\nŁ \\nCFO, OTPP\\nŁ \\nŁ \\n\\n\\nŁ \\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\nAuthority and scope of \\n the framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 8\\n\\n\\n\\n\\n\\nRisks created by climate change:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUncertainty of future outcomes and scenarios: \\nIn 2015, at the United Nations \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n As \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1The Climate Action Tracker\\n\\n\\n\\n\\n\\n\\n\\nWHAT IS CLIMATE CHANGE?\\n1 Climate Action Tracker: Global emissions time series, October 2020\\nIntroduction\\nŁ \\nCFO, OTPP\\nŁ \\nŁ \\n\\n\\nŁ \\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\nAuthority and scope of \\n the framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 9PROACTIVELY ADDRESSING CLIMATE CHANGE AND ITS POTENTIAL IMPACT CAN LEAD TO SHAREHOLDER VALUE\\n\\n\\n\\n\\n2WHAT IS THE LINK BETWEEN CLIMATE CHANGE AND \\nBUSINESS VALUATION?\\nExamples of the potential impact of actively addressing climate change\\n2 https://www.intactcentreclimateadaptation.ca/wp-content/uploads/2020/03/Factoring-Climate-Risk-into-Financial-Valuation.pdf\\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\nOperating \\ndecisions\\nŁ \\nŁ \\nŁ \\nŁ \\nInvestment \\n\\ndecisions\\nŁ \\nŁ \\nFinancing \\n\\ndecisions\\n\\noperations\\nDiscount rate\\nCreating \\nshareholder\\n value\\nDebt\\nIntroduction\\nŁ \\nCFO, OTPP\\nŁ \\nŁ \\n\\n\\nŁ \\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\nAuthority and scope of \\n the framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\n10\\nWHAT IS THE LINK BETWEEN CLIMATE CHANGE AND \\nBUSINESS VALUATION?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5Operating \\n\\ndecisions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n76Financing \\n\\ndecisions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInvestment \\n\\ndecisions\\nCREATING SHAREHOLDER VALUE\\nShareholder value is improved by making \\n\\n\\n\\ndecisions. Investors can create resilient, stronger \\n\\n\\n\\nenvironmental risks, which can then trigger \\n\\ncapital reallocation and asset repricing. This, in \\n\\nturn, offers opportunities for long-term investors \\n\\nsuch as pension funds. Companies that exhibit \\n\\nbetter environmental, social and governance \\n\\n(ESG) traits tend to:\\nŁOutperform their peers (ie companies focusing\\n\\n\\nto their industry tend to perform well)\\n8ŁAchieve a lower cost of capital\\n9ŁMinimize share price volatility\\n10On the other hand, a company that has an \\n\\nunsustainable business model or that responds \\n\\nweakly to climate change may face diminished \\n\\nreturns and may even be subject to stranded-\\n\\nasset risk. Over time, companies and countries \\n\\nthat do not respond to stakeholder demands \\n\\nand address sustainability risks may encounter \\n\\ngrowing scepticism from the markets and, in turn, \\n\\nbe subject to a higher cost of capital.\\nhttps://www.osc.gov.on.ca/documents/en/Securities-Category5/csa_20190801_51-358_reporting-of-climate-change-related-risks.pdf\\nhttps://www.intactcentreclimateadaptation.ca/wp-content/uploads/2020/03/Factoring-Climate-Risk-into-Financial-Valuation.pdf\\n5 \\nvaluations\\n6 https://www2.deloitte.com/us/en/insights/topics/strategy/impact-and-opportunities-of-climate-change-on-business.html7 \\n\\n\\n10 https://yoursri.com/media-new/download/jpm-esg-how-esg-can-enhance-your-portfolio.pdf\\nA4S Essential Guide to Valuations and Climate Change\\n Introduction\\nŁ\\nCFO, OTPP\\nŁ\\nŁ\\n\\n\\nŁ\\nŁ\\n\\n\\n\\n\\nŁ\\n\\n\\nAuthority and scope of \\nthe framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 11\\nTo date the \\ngeneral approach to considering climate change in valuations has \\nbeen qualitative in nature\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ Lack of comparable information:\\n\\n\\n\\n\\nŁ Limited published scenarios:\\n\\n\\nŁ Uncertainty: \\n\\n\\n\\n\\n\\nŁ Empirical evidence:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCURRENT APPROACH TO CLIMATE CHANGE IN VALUATIONS\\nIntroduction\\nŁ \\nCFO, OTPP\\nŁ \\nŁ \\n\\n\\nŁ \\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\nAuthority and scope of \\n the framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 12\\nTitle of guide to go here\\n 12\\nA4S Essential Guide to Valuations and Climate Change\\n 12\\nAUTHORITY AND \\nSCOPE OF THE \\n\\nFRAMEWORK\\nIntroduction\\nAuthority and scope of  \\nthe framework\\nŁ \\n\\nŁ \\n\\nlimitations\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 13\\n\\n\\n\\n\\nWHO ARE THE KEY STAKEHOLDERS?\\n\\nFINANCIAL SYSTEMAsset OwnersPension FundsInstitutional InvestorsInsurance CompaniesRetail BanksInvestmentConsultantsAsset ManagersBrokers\\nFinancial AdvisersRetail BanksNon EquityInvestment BanksCompaniesDebtEquityProjects / Assets\\nInvestments & OperationsStock ExchangeFinancial RegulationCredit Rating \\nAgencies (CRA)Data Providers\\nFinancial MediaOther RegulationsSustainability ImpactsIndividualsBuy-SideSell-SideBondsPrivate EquityProperty\\nCommoditiesCurrency\\nDerivativesConsumersWorkers\\nInvestors\\nValuation \\npractitioners \\n\\nand valuation \\n\\nauthoritative \\n\\nbodies\\nRegulators \\n\\nincluding \\n\\naccounting \\n\\nand valuation \\n\\nstandard \\n\\nsetters\\nFinancial \\n\\nInstitutions\\nFinancial \\n\\nstatement \\n\\nauditors\\nIntroduction\\nAuthority and scope of  \\nthe framework\\nŁ \\n\\nŁ \\n\\nlimitations\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 14\\nVALUATION PRACTITIONERS AND VALUATION AUTHORITATIVE BODIES\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1211\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nINVESTORS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWHO ARE THE KEY STAKEHOLDERS?\\n11 https://www.cnn.com/2019/12/09/economy/climate-change-company-valuations/index.html\\n12 \\n\\nvaluations\\n\\n\\n\\nIntroduction\\nAuthority and scope of  \\nthe framework\\nŁ \\n\\nŁ \\n\\nlimitations\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n15\\nFINANCIAL INSTITUTIONS\\n\\n\\n\\n\\n\\n\\n\\n\\n15\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFINANCIAL STATEMENT AUDITORS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© and \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppendix 3\\n\\n\\nWHO ARE THE KEY STAKEHOLDERS?\\n15 \\n\\n16 \\nhttps://www.cnbc.com/2020/02/14/esg-investing-numbers-suggest-green-investing-mega-trend-is-here.html\\nIntroduction\\nAuthority and scope of  \\nthe framework\\nŁ\\n\\nŁ\\n\\nlimitations\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n16\\nREGULATORS INCLUDING ACCOUNTING AND VALUATION STANDARD SETTERS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ncall to action\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWHO ARE THE KEY STAKEHOLDERS?\\nIntroduction\\nAuthority and scope of  \\nthe framework\\nŁ \\n\\nŁ \\n\\nlimitations\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n17\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nINTENDED BENEFITS AND LIMITATIONS\\nThe framework does this by:\\nŁ\\nŁ\\nŁ\\n\\n\\n\\n\\nŁ\\n\\n\\nŁ\\n\\n\\n\\nThe framework does not:\\nŁ\\n\\n\\n\\nŁ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\nAuthority and scope of  \\nthe framework\\nŁ\\n\\nŁ\\n\\nlimitations\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n18\\nTitle of guide to go here \\n18\\nA4S Essential Guide to Valuations and Climate Change \\n18\\nCLIMATE CHANGE \\nVALUATION \\n\\nFRAMEWORK\\nIntroduction\\nAuthority and scope of \\nthe framework\\nClimate change valuation \\nframework\\nŁ\\nŁ\\nŁAssess\\nŁ\\nŁ\\nŁ\\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n19\\n\\n\\n\\n\\n\\n\\nOVERVIEW\\nTHE FIVE-STEP CLIMATE CHANGE VALUATION FRAMEWORK\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVALUATION TOOL TIP\\nIdentify\\n\\nAssess\\n\\n\\nFilter\\n\\n\\n\\nIntegrate\\n\\n\\n\\n\\nTriangulate\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSchedule A\\n\\n\\nSchedule B\\n\\n\\nSchedule C\\n\\nSchedule D\\n\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\nValuations and Climate Change\\n\\nExcel-based tool:\\n This section should be read in conjunction \\n\\nwith the Valuations and Climate Change \\n\\nExcel-based tool, which can be found here:\\nwww.accountingforsustainability.org/\\nvaluations\\nRefer to the schedules found in the Excel tool\\nto help perform the assessment.\\n Introduction\\nAuthority and scope of \\nthe framework\\nClimate change valuation \\nframework\\nŁ\\nŁ\\nŁAssess\\nŁ\\nŁ\\nŁ\\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 20\\nOVERVIEW\\nTHE FIVE-STEP CLIMATE CHANGE VALUATION FRAMEWORK\\nLikelihood\\nMateriality\\nLow\\nModerate\\nHigh\\nVery high\\n\\nMarket valuation\\nDiscount rate\\nvs.\\n\\nPeer A\\nPeer B\\nPeer C\\nPeer D\\nEntity valuation\\nOpportunities\\nRisks\\nIdentify\\nKey business drivers\\nCore activities\\nOperating environment\\nGeographies\\nRevenue / costs\\nPolicy\\n and legal\\nResource \\n\\nTechnology\\nEnergy\\n source\\nMarket\\nProduct / \\nservices\\nReputation\\nAcute\\nChronic\\nMarkets\\nResilience\\n+xTriangulate\\nIntegrate\\nFilter\\nAssess\\nIntroduction\\nAuthority and scope of \\nthe framework\\nClimate change valuation \\nframework\\nŁ\\nŁ\\nŁAssess\\nŁ\\nŁ\\nŁ\\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n21\\nA4S Essential Guide to Valuations and Climate Change \\n21\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIDENTIFY\\nSTART BY IDENTIFYING THE KEY VALUE DRIVERS OF THE COMPANY / ASSET\\nBusiness / asset\\nPossible consideration areas\\nCore activities\\nŁ\\nŁ\\nŁ\\nŁ\\n\\n\\nŁ\\nOperating\\n environment\\nŁ\\nŁ\\nŁ\\nGeographie\\nsŁ\\nŁ\\nŁ\\nRevenue / co\\nsts\\nŁ\\nŁ\\nŁ\\nŁ\\n\\n\\nIntroduction\\nAuthority and scope of \\nthe framework\\nClimate change valuation \\nframework\\nŁ\\nŁ\\nŁAssess\\nŁ\\nŁ\\nŁ\\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n22\\nA4S Essential Guide to Valuations and Climate Change \\n22\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\n\\n\\n\\n\\nAppendix 5\\nWASTE WATER MANAGEMENT (WWM)\\nCompany background\\nŁ \\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\nIDENTIFY: CASE STUDY\\nRelevant case information about WWM\\nRegulatory framework\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\nDam levels\\nŁ \\nDemand Managemen\\nt Plan \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\nEnergy costs\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNOTICE TO READER\\n\\n\\n\\n\\n\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nŁ \\nŁ \\nŁ Assess\\nŁ \\nŁ \\nŁ \\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n23\\nA4S Essential Guide to Valuations and Climate Change \\n23\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\nASSESS: POSSIBLE SOURCES OF COMPANY INFORMATION\\n\\n\\n\\n\\nDiscussions with management\\nCompany management are closest to the day-to-day operations \\nand have a deep understanding of the risks and opportunities faced\\n\\nby both the industry and the company. Discussions with \\n\\nmanagement will help prioritize risks and opportunities \\n\\nand provide detail of how they may impact the business \\n\\nvaluation.\\n Sources of \\ninformation\\nDiscussions with \\nmanagement\\nExt\\nproviders\\nEquity-analyst \\nreports and \\ncredit rating \\nagencies\\nAsset-level \\ndata\\n\\nsustainability \\nreporting\\nCorporate \\nreporting\\nEquity-analyst reports and credit rating agencies\\nAsset-level data\\nAsset-level and geospatial data can be an important source of\\ninformation, in particular for an assessment of physical risk, but \\ncan also provide emissions information that helps to close company \\ndata gaps and enable a more granular assessment. Examples of projects \\nfocused on the provision of data in this area include the Spatial Finance \\n\\nInitiative at the University of Oxford.\\nExternal data providers\\nVarious external data providers are beginning to summarize and disclose \\n\\nclimate change risk-related data, with regard to physical and transition risks, \\nprimarily for publicly listed entities and for the industries most impacted\\nby climate change. These data providers may give insight into risks \\nand opportunities faced by the subject company with regard \\nto the public comparables and the broader industries and \\ngeographies that may be relevant to the valuation.\\nernal data \\n Corporate reporting\\nSome companies disclose climate change \\n\\ndata as part of their annual or sustainability \\n\\nreporting. Reviewing this reporting can \\n\\ngive an indication of which climate change \\n\\nrisks and opportunities management deem \\n\\n\\n\\ncommentary may give details on any \\n\\nmitigating actions being taken to manage \\n\\nthe risks.\\nPublic companies may have equity-analyst reporting\\navailable, wh\\nich may provide an independent view \\nof the climate risks and opportunities faced by the \\n\\ncomparable companies and the broader industry. \\nCredit rating agencies have begun to embed ESG \\nrisk into ratings. In some cases, this has led to \\ndowngrades, including ratings reductions as a result \\nof physical or transition risks.\\n \\nAs with the above, company disclosures comparable \\n\\nat the sector level can assist in determining what \\n\\nrisks and opportunities companies may face. Sector-\\n\\n\\n\\nthose comparable companies are similar or dissimilar to the subject \\n\\ncompany. This information may help inform a market valuation \\n\\nmethodology (ie market multiples), if applicable.\\n Introduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nŁ \\nŁ \\nŁ Assess\\nŁ \\nŁ \\nŁ \\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n24\\nA4S Essential Guide to Valuations and Climate Change \\n24\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\nThere are various frameworks and standards available that underpin the way most companies report on climate-related risks, opportunities and impacts. A summary \\nof the most widely adopted frameworks and the kind of information that can be derived from each is summarized below. Please see \\nAppendix 4\\n for further information.\\nASSESS: THE CLIMATE REPORTING LANDSCAPE\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWe expect that over time \\n\\nconvergence will occur and companies, either \\n\\nthrough government regulations or on their own \\n\\ninitiative, will begin to disclose information based \\n\\n\\n\\nshould lead to improved disclosures and facilitate \\n\\nthe extraction of climate change risk data from \\n\\n\\n\\n\\n\\n\\n\\n\\nSustainability Accounting Standards Board (SASB)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDISCLOSURE INSIGHT ACTIONCDP\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTask Force on Climate-related Financial\\n\\nDisclosures (TCFD)\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGlobal Reporting Initiative (GRI) and the Global\\n\\nSustainability Standards Board (GSSB)\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGreenhouse Gas Protocol\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nŁ \\nŁ \\nŁ Assess\\nŁ \\nŁ \\nŁ \\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n25\\nA4S Essential Guide to Valuations and Climate Change \\n25\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\n\\n\\n\\n\\n\\nto \\nAppendix 4\\n\\n17ASSESS: LENSES TO ASSESS CLIMATE CHANGE RISKS\\nAND OPPORTUNITIES\\n Description\\nValue considerations\\n Transition Risks Œ \\nPolicy\\n\\n\\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\n Transition Risks Œ \\nLegal\\n\\n\\n\\n\\n\\nŁ \\nŁ \\n Transition Risks Œ \\nTechnology\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ \\nŁ \\nŁ Changes in demand\\nŁ \\nŁ \\n\\n\\nFurther detail on what to consider can be found in \\nAppendix 1\\n.Introduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nŁ \\nŁ \\nŁ Assess\\nŁ \\nŁ \\nŁ \\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n26\\nA4S Essential Guide to Valuations and Climate Change \\n26\\nASSESS: LENSES TO ASSESS CLIMATE CHANGE RISKS \\nAND OPPORTUNITIES\\nDescription\\nValue considerations\\n Transition Risks Œ \\nMarket\\n\\n\\nŁ \\nŁ \\n\\nŁ \\nŁ \\nŁ Changes in demand\\nŁ \\n\\n\\nŁ \\n\\n\\nŁ \\n\\n Transition Risks Œ \\nReputation\\n\\n\\n\\n\\n\\n\\nŁ \\nŁ \\nŁ \\n\\nŁ \\nŁ \\n\\n\\nFurther detail on what to consider can be found in \\nAppendix 1\\n.Introduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nŁ \\nŁ \\nŁ \\nAssess\\nŁ \\nŁ \\nŁ \\n\\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 27\\nA4S Essential Guide to Valuations and Climate Change\\n 27\\nASSESS: LENSES TO ASSESS CLIMATE CHANGE RISKS \\nAND OPPORTUNITIES\\nDescription\\nValue considerations\\n Physical Risks Œ Chronic\\n\\nŁ \\nŁ \\nŁ \\n\\nŁ \\nŁ \\n\\nŁ \\nŁ \\nŁ \\n\\nŁ \\n\\nŁ \\n\\nŁ \\n\\nŁ \\n\\n\\n\\nŁ \\n Physical Risks Œ Acute\\n\\n\\nŁ Flooding\\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\n\\nŁ \\n\\n\\nŁ \\nŁ \\n\\n\\nŁ Changes in demand\\nFurther detail on what to consider can be found in \\nAppendix 1\\n.\\nA4S Essential Guide to Valuations and Climate Change\\n 28\\nA4S Essential Guide to Valuations and Climate Change\\n 28\\n\\n\\nASSESS: LENSES TO ASSESS CLIMATE CHANGE RISKS \\nAND OPPORTUNITIES\\nDescription\\nValue considerations\\n Opportunities Œ \\nProducts and services\\n\\nŁ \\n\\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\n\\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\n Opportunities Œ \\nMarkets\\n\\nŁ \\nŁ \\nŁ \\nŁ \\n\\n\\n\\nŁ \\n\\n\\n Opportunities Œ \\nResilience\\n\\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\n\\nŁ \\n\\n\\nŁ \\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 29\\nA4S Essential Guide to Valuations and Climate Change\\n 29\\nASSESS: LENSES TO ASSESS CLIMATE CHANGE RISKS \\nAND OPPORTUNITIES\\nDescription\\nValue considerations\\n Opportunities Œ \\n\\n\\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\n\\nŁ \\n\\n Opportunities Œ \\nEnergy source\\n\\nŁ \\nŁ \\nŁ \\nŁ \\n\\nŁ \\nŁ \\nŁ \\nŁ \\n\\nŁ \\n\\n\\nŁ \\n\\nŁ \\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 30\\nA4S Essential Guide to Valuations and Climate Change\\n 30\\nAssessed \\nrisks and \\nopportunities \\n for WWM\\nOpportunity: \\nDam levels\\nMarket risk:\\n Long-term \\nenergy \\ncontracts\\nReputation \\nrisk:\\n Owner \\nreputation\\nAcute physical \\nrisk: \\nExtreme \\nweather\\nPolicy risk: \\nWater-supply \\nregulation\\nChronic \\nphysical risk: \\nRising sea \\nlevels\\nASSESS: WWM CASE STUDY\\nRising sea levels\\nGiven Staten Island is bordered by a body of water, it is susceptible to the \\nimpact of rising sea levels caused by climate change. Although the timing \\n\\nand intensity of rising sea levels is unknown, WWM™s geographic location \\n\\nmakes it susceptible to this chronic physical risk.\\nWater supply regulations\\nThe current regulatory period ends in 2023, and the regulatory framework \\n\\n\\ncontinues to increase the strain on NYC™s drinking water supply, the \\n\\n\\n\\n\\n\\ncontemplated reduction of the regulated return on WWM™s asset base. \\n\\n\\n\\ncritical role WWM plays in NYC™s drinking water supply network. \\n\\n\\n\\nreturn.\\nOwner reputation\\nThere is the potential that WWM, and its shareholders, may be viewed \\n\\n\\n\\n\\nadds incremental risk to the business, as there is potential backlash from \\n\\nregulators, the government and the population that WWM serves, among \\n\\nother stakeholders, which has the potential to ultimately lead to lower \\n\\nregulatory rates.\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 31\\nA4S Essential Guide to Valuations and Climate Change\\n 31\\nASSESS: WWM CASE STUDY\\nDam levels\\nDam levels have been falling at unprecedented rates with current dam levels at \\n50% of capacity. Climate change is expected to continue to increase the length \\n\\nand severity of droughts. WWM has received notice from the regulator to turn \\n\\non the plant by the end of 2020. Given WWM has just completed the rebuild and \\n\\ntesting of the plant following damages caused by a recent tornado, management \\n\\nexpects little problem in bringing operations to full capacity by the end of 2020.\\nExtreme weather\\n\\ndamage to the WWM plant and minor personnel injuries. There was no impact \\n\\nto plant operations as WWM was in shutdown mode. Immediately following \\n\\nthe tornado, management began its efforts to rebuild the plant. Management \\n\\nbudgeted US$80 million of rebuild costs including contingency over the span of \\n\\ntwo years. Further, given the tornado was a natural disaster covered by WWM™s \\n\\nproperty and casualty insurance, WWM received a settlement of US$75 million in \\n\\n2019, which substantially covered the rebuild costs.\\nLong-term energy contracts\\n\\n\\n\\n\\n\\nprice forecasts. This uncertainty is exacerbated by more renewable power \\n\\nexpected to come online as the state transitions away from fossil-fuel-based \\n\\npower generation. Greater supply and lower prices for this energy are expected.\\nAssessed \\nrisks and \\nopportunities \\n for WWM\\nAcute physical \\nrisk: \\nExtreme \\nweather\\nChronic \\nphysical risk: \\nRising sea \\nlevels\\nOpportunity: \\nDam levels\\nReputation \\nrisk:\\n Owner \\nreputation\\nMarket risk:\\n Long-term \\nenergy \\ncontracts\\nPolicy risk: \\nWater-supply \\nregulation\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 32\\nA4S Essential Guide to Valuations and Climate Change\\n 32\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppendix 2\\n\\n\\n\\n\\nŁ \\n\\nŁ \\nŁ \\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFILTER: LIKELIHOOD AND MATERIALITY ASSESSMENT\\n\\n\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nŁ \\nŁ \\nŁ Assess\\nŁ \\nŁ \\nŁ \\nAppendices 1-4\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 33\\nA4S Essential Guide to Valuations and Climate Change\\n 33\\nFILTER: LIKELIHOOD AND MATERIALITY ASSESSMENT\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 34\\nA4S Essential Guide to Valuations and Climate Change\\n 34\\nWater supply \\nregulations:\\n\\n\\n\\n\\n\\n\\n\\n\\nLong-term energy \\n\\ncontracts:\\n\\n\\n\\n\\n\\n\\nOwner reputation:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nExtreme weather:\\n\\n\\n\\n\\n\\n\\n\\n\\nRising sea levels:\\n\\n\\n\\n\\n\\n\\nDam levels:\\n\\n\\n\\n\\n\\n\\nType of risk / opportunity\\nDetail\\nRating\\nTransition riskTransition riskTransition risk\\nPhysical riskPhysical risk\\nOpportunity\\nPolicy\\nMarket\\nReputation\\nAcute\\nChronic\\nResource \\n\\nHigh\\nLikelihood\\nMateriality\\nLow\\nLikelihood\\nMateriality\\nHigh\\nLikelihood\\nMateriality\\nModerate\\nLikelihood\\nMateriality\\nModerate\\nLikelihood\\nMateriality\\nModerate\\nLikelihood\\nMateriality\\nRisks\\nOpportunity\\nPolicy\\nTechnology\\nResource \\n\\nMarket\\nEnergy \\nsource\\nReputation\\nProduct / \\nservices\\nAcute\\nMarkets\\nChronic\\nResilience\\nFILTER: WWM CASE STUDY\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 35\\nA4S Essential Guide to Valuations and Climate Change\\n 35\\nINTEGRATE: ASSESS WHERE IN THE DCF TO ADJUST FOR \\nRISKS AND OPPORTUNITIES\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdjust discount rate\\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\nŁ \\n\\n\\nŁ \\n\\nŁ \\n\\n\\nŁ \\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 36\\nA4S Essential Guide to Valuations and Climate Change\\n 36\\nŁ Lower regulated \\nreturn based on \\nnew rates provided\\nŁ Additional risk \\n\\nthat rates may \\n\\ndecrease further \\n\\nand be captured in \\n\\na premium\\nŁ Volume and price \\n\\n\\n\\n\\nconsidered in cash \\n\\n\\nŁ Uncertainty in \\n\\n\\n\\nprice captured in a \\n\\npremium\\nŁ Lack of reliable \\n\\n\\nŁ Risk premium \\nbecause of lack \\n\\n\\n\\nadjustments\\nŁ Increased \\n\\ninsurance \\n\\npremiums included \\n\\n\\nŁ Additional risk \\n\\ninsurance may \\n\\nnot be available to \\n\\nbe captured in a \\n\\npremium\\nŁ Lack of reliable \\n\\n\\nŁ Risk premium \\nbecause of lack \\n\\n\\n\\nadjustments\\nŁ Adjusted cash \\n\\n\\ncapacity generated \\n\\nfrom dam levels\\nType of risk / opportunity\\nRating\\nTransition riskTransition riskTransition risk\\nPhysical riskPhysical risk\\nOpportunity\\nPolicy\\nMarket\\nReputation\\nAcute\\nChronic\\nResource \\n\\nHigh\\nLow\\nHigh\\nModerate\\nModerate\\nModerate\\nRisks\\nOpportunity\\nPolicy\\nTechnology\\nResource \\n\\nMarket\\nEnergy \\nsource\\nReputation\\nProduct / \\nservices\\nAcute\\nMarkets\\nChronic\\nResilience\\nINTEGRATE: CASE STUDY\\n\\nA4S Essential Guide to Valuations and Climate Change \\n37\\nA4S Essential Guide to Valuations and Climate Change \\n37\\nINTEGRATE: CASE STUDY\\nAlthough in this case study \\nthe value has decreased, \\n\\nthis will not be the case in \\n\\nall instances. As climate \\n\\nchange is a business risk, \\n\\nsome valuations may already \\n\\n\\n\\nchange. In some cases it may \\n\\npresent an opportunity and \\n\\ntherefore increase the value of \\n\\na business.\\nValuation ignoring climate change\\nPercent\\nFY + 1\\nFY + 2\\nFY + 3\\nFY + 4\\nFY + 5\\nTerminal\\n\\n100\\n110\\n120\\n\\n\\n150\\n\\n\\n100\\n110\\n120\\n\\n\\n150\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n9697\\n97\\n96951,454\\nFair value\\n1,934\\nValuation considering climate change\\nPercent\\nFY + 1\\nFY + 2\\nFY + 3\\nFY + 4\\nFY + 5\\nTerminal\\n\\n100\\n110\\n120\\n\\n\\n150\\n\\n\\nŁ Lower return on regulated rates\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\nŁ \\nInsurance premiums\\n\\n\\n\\n\\n\\n\\nŁ Extra capacity\\n10111215\\n101\\n111\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n89\\n888886851,172\\nFair value\\n1,611\\n\\nA4S Essential Guide to Valuations and Climate Change \\n38\\nA4S Essential Guide to Valuations and Climate Change \\n38\\nAdjust the multiple\\n\\nINTEGRATE: ASSESS WHETHER ADJUSTMENTS IN \\nMULTIPLES ARE REQUIRED\\n\\nŁ \\n\\n\\n\\n\\n\\n\\nŁ Geographical impacts that account for \\n\\nclimate change\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSASB materiality map\\n\\n\\n\\n\\n\\n\\n\\nSample Financial Metrics\\nŁ \\nŁ \\nSample Activity-based Metrics\\nŁ \\nŁ \\nŁ \\nŁ \\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAssess the comparability of the risks and \\n\\n\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change \\n39\\nA4S Essential Guide to Valuations and Climate Change \\n39\\nINTEGRATE: WWM CASE STUDY\\n\\n\\n\\n\\n\\n\\n\\n\\n US$1,500\\nType\\nCALIFORNIA WATER \\n\\nMANAGEMENT\\nProvides water utility and other \\nrelated\\n services in California.\\nALABAMA WATER GROUP\\n\\nProvides water and electric \\nservices\\n to residential, industrial \\nand other customers in Alabama.\\nWWM\\nSector\\nUtilities\\nUtilities\\nUtilities\\nGeography\\nUnited States Œ California\\nUnited States Œ Alabama\\nUnited States Œ New York\\nScope 1 and 2 emissions\\n0.55 Mt CO\\n2e0.70 Mt CO\\n2e0.60 Mt CO\\n2eEmissions / Revenue\\n600 tCO\\n2e / US$M\\n650 tCO\\n2e / US$M\\n625 tCO\\n2e / US$M\\nEnterprise Value / EBITDA\\n10.0x\\n10.0x\\n10.0x\\n\\nA4S Essential Guide to Valuations and Climate Change \\n40\\nA4S Essential Guide to Valuations and Climate Change \\n40\\nTRIANGULATE\\nAdditional considerations\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDCF approach\\nMarket dislocation\\nConcluded value\\n*Market approach\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change \\n41\\nA4S Essential Guide to Valuations and Climate Change \\n41\\nTRIANGULATE: CASE STUDY\\nAdditional considerations\\nMarket\\n\\n\\n\\n\\nApplication\\n\\n\\n\\n\\n\\n\\n\\nMarket dislocation\\n\\n\\n\\n\\n\\n\\n\\nUpside and downside\\n\\n\\n\\n\\n\\nDouble counting\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVALUE CONCLUSION\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConcluded value US$1,550\\nDCF value US$1,600\\nMarket value US$1,500\\nMarket considerations\\n\\nA4S Essential Guide to Valuations and Climate Change \\n 42\\nTitle of guide to go here\\n42\\nA4S Essential Guide to Valuations and Climate Change\\n 42\\nAPPENDIX 1\\n\\nA4S Essential Guide to Valuations and Climate Change \\n43\\nAPPENDIX 1: FURTHER DETAIL ON CLIMATE CHANGE \\nFACTORS\\n\\nPolicy lens\\nGovernments will focus on minimizing the impact of climate change, both domestically and globally, and will drive new laws and policy initiatives that seek to restrict negative contributors \\nand to promote adaptation to climate change\\nRisks\\nŁ  \\n    \\n\\nŁ\\n\\nŁ\\n\\n\\n\\nŁ\\n\\n\\n\\nOpportunities\\nŁ\\n\\n\\nMitigation\\nŁ\\n\\n\\n\\nRegulatory environment\\n\\nŁ \\nŁ \\nŁ \\n\\n\\n\\n\\nŁ \\nŁ \\nŁ \\nŁ \\nManagement\\n\\n\\n\\n\\nMega \\nthemes\\nKey value  \\nconsiderations\\n\\nA4S Essential Guide to Valuations and Climate Change \\n44\\nAPPENDIX 1: FURTHER DETAIL ON CLIMATE CHANGE \\nFACTORS\\n\\n\\n\\n\\nLegal lens\\nIncreased consumer, investor and government awareness of climate change\\nRisks\\nŁ \\n\\n\\nŁ \\nŁ \\n\\n\\nŁ \\n\\n\\n\\nOpportunities\\nŁ \\nMitigation\\n\\nŁ \\nŁ \\nŁ \\n\\n\\n\\nBusiness operations\\nŁ \\n\\n\\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\nŁ \\n\\nSocio-political pressure to hold responsible companies to account\\nManagement / governance\\nŁ \\nŁ \\nŁ \\nMega \\nthemes\\nKey value  \\nconsideration\\ns\\nA4S Essential Guide to Valuations and Climate Change \\n45\\nAPPENDIX 1: FURTHER DETAIL ON CLIMATE CHANGE \\nFACTORS\\n\\n\\n\\n\\n\\n\\nTechnology lens\\nTechnological advancements / innovation in energy, \\nproduction and transport\\nRisks\\nŁ \\n\\n\\nŁ \\n\\n\\n\\nŁ \\n\\n\\n\\nOpportunities\\nŁ \\n\\n\\nŁ \\n\\n\\nMitigation\\n\\nŁ \\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\nTechnological innovation\\n\\n\\nŁ \\n\\n\\nŁ \\n\\n\\n\\nŁ \\nŁ \\nŁ \\nPotential for redundancy of systems or products\\nCreative destruction\\nŁ \\n\\n\\n\\nThe timing of technology development and deployment \\n\\nis a key uncertainty\\nManagement\\nŁ \\n\\n\\n\\n\\n\\nMega \\nthemes\\nKey value  \\nconsiderations\\n\\nA4S Essential Guide to Valuations and Climate Change \\n46\\nAPPENDIX 1: FURTHER DETAIL ON CLIMATE CHANGE \\nFACTORS\\n\\n\\nMarket lens\\nIncreased consumer, investor and government \\nawareness of climate change\\nRisks\\nŁ \\n\\nŁ \\n\\n\\nŁ \\n\\n\\nŁ \\n\\n\\nOpportunities\\nŁ \\n\\n\\nŁ \\n\\nMitigation\\nŁ \\n\\nŁ \\n\\n\\nDemand\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\nŁ \\nŁ \\n\\n\\nŁ \\n\\n\\nChanging operating environment, including the \\n\\navailability and cost of production\\nSupply\\n\\n\\n\\n\\nŁ \\nŁ \\nŁ \\n\\n\\nŁ \\n\\nŁ \\n\\n\\n\\n\\n\\n\\nresources\\nManagement\\nŁ \\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\nMega \\nthemes\\nKey value  \\nconsiderations\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 47\\nAPPENDIX 1: FURTHER DETAIL ON CLIMATE CHANGE \\nFACTORS\\n\\n\\n\\n\\nReputation lens\\nShifts in investor preferences due to widespread application of climate-related exclusions, shareholder activism, regulatory / \\npublic pressure and rising cost of capital for high emitters\\nRisks\\nŁ \\n\\nŁ \\n\\nŁ \\nOpportunities\\nŁ \\n\\n\\n\\n\\nMitigation\\nŁ \\n\\n\\n\\nExposure\\nŁ \\nŁ \\n\\nŁ \\nŁ \\nComplex and potentially dramatic shifts in \\n\\necosystems are expected\\nMega \\nthemes\\nKey value  \\nconsiderations\\nManagement\\nŁ \\n\\n\\n\\nŁ \\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 48\\nAPPENDIX 1: FURTHER DETAIL ON CLIMATE CHANGE \\nFACTORS\\n\\n\\n\\n\\nChronic \\n physical lens\\nChanging environmental factors will impact society \\nand the economy via multiple channels.\\nRisks\\nŁ \\n\\n\\nŁ \\n\\n\\nŁ \\n\\n\\nOpportunities\\nŁ \\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\nMitigation\\nŁ \\n\\nŁ \\n\\n\\nŁ \\nExposure\\n\\n\\nŁ \\n\\nŁ \\nŁ \\nŁ \\nŁ \\n\\n\\nŁ \\n\\n\\n\\n\\n\\nŁ \\n\\n\\nŁ \\n\\n\\nŁ \\n\\n\\nŁ \\n\\n\\nŁ \\n\\n\\nComplex and potentially dramatic shifts \\n\\nin ecosystems are expected.\\nUncertainty over the timing and speed of change.\\nManagement\\nŁ \\n\\n\\n\\n\\n\\nŁ \\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\nMega \\nthemes\\nKey value  \\nconsiderations\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 49\\nAPPENDIX 1: FURTHER DETAIL ON CLIMATE CHANGE \\nFACTORS\\n\\n\\nAcute physical lens\\nThe incidence and severity of extreme weather events is expected to increase as the world warms.\\nRisks\\nŁ \\nŁ \\n\\nŁ \\n\\nOpportunities\\nŁ \\n\\n\\n\\nŁ \\n\\n\\nMitigation\\nŁ \\n\\n\\n\\n\\n\\nExposure\\n\\nŁ \\nŁ \\nŁ \\nŁ \\nŁ Changes in demand\\n\\nŁ \\nŁ \\nŁ \\nŁ \\nManagement\\n\\nŁ \\n\\n\\n\\nMega \\nthemes\\nKey value\\n considerations\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 50\\nTitle of guide to go here\\n 50\\nA4S Essential Guide to Valuations and Climate Change\\n 50\\nAPPENDIX 2\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 51\\nAPPENDIX 2: EXTERNAL DATA PROVIDERS\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ Bloomberg\\nŁ The Climate Service\\nŁ Four Twenty Seven\\nŁ MSCI\\nŁ Trucost\\nNOTICE TO READER\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 52\\nTitle of guide to go here\\n 52\\nA4S Essential Guide to Valuations and Climate Change\\n 52\\nAPPENDIX 3\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 53\\nAPPENDIX 3: RELEVANCE TO FINANCIAL REPORTING\\n\\n\\n\\n\\n\\n\\n\\n\\nI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClimate-related and \\n\\n\\n\\nAASB / IASB Practice Statement 2\\n\\n\\n\\n\\nŁ \\nŁ \\nŁ \\nŁ \\n\\n\\n\\nŁ \\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIAASB: The Consideration of Climate-Related Risks in an Audit of Financial \\nStatement\\n\\n https://cdn.ifrs.org/-/media/feature/news/2019/november/in-brief-climate-change-nick-anderson.pdf?la=en\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 54\\nTitle of guide to go here\\n 54\\nA4S Essential Guide to Valuations and Climate Change\\n 54\\nAPPENDIX 4\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 55\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEstimated Scope (Energy)Vehicle Sales (Transportation)GHG Emissions (Materials and Buildings)Mechanical Emissions (Agriculture, Food and Forest Products)Fuel Consumption (Transportation)  CDP-SASBCDPCDPCDPCDPCDPRevenues(Energy)EEDI(Transportation)Area of Buildings (Materials and Buildings)Average Carbon  (Financial Services)Life Cycle (Transportation)  SASB-CDPCDPCDPCDPCDPCDPProportion(Energy)Expenditures (Transportation)Reserve Breakdown (Materials and Buildings)Absolute Carbon Emissions (Financial Services)Expenditures (Materials and Buildings)CDP-GRI, CDP-SASB, SASB-CDPCDPCDPCDPCDPCDPGross Amount(Energy)Road Vehicles (Transportation)(Materials and Buildings)Portfolio Carbon Emissions (Financial Services)Energy Intensity (Materials and Buildings)CDP-GRI, CDP-SASB, SASB-CDPCDPCDPCDPCDPCDPIndicative Costs (Energy)Investment (Transportation)Revenues (Agriculture, Food and Forest Products)Volume of Portfolio Carbon(Financial Services)Fresh Water Percentage(Materials and Buildings)CDP-SASB, SASB-CDPCDPCDPCDPCDPCDPAssets(Energy)Revenues (Materials and Buildings)Water Withdrawn (Agriculture, Food and Forest Products)GHG Emissions(All)CDP-SASB, GRI-SASB, SASB-CDP, SASB-GRIInvestment(Materials and Buildings)CDP-GRI, CDP-SASB, SASB-CDPCDPCDPCDPCDPCDPReserves Breakdown (Energy)Energy Consumption (Materials and Buildings)Water Percentage (Agriculture, Food and Forest Products)Carbon Prices (Energy)  SASB-CDPExpenditures (Agriculture, Food and Forest Products)CDP-GRI, GRI-CDPCDPCDPCDPCDPCDPCapital Payback (Energy)Fuel Consumption (Materials and Buildings)Assets(Agriculture, Food and Forest Products)Expenditure Low Carbon(Energy)CDP-SASB, SASB-CDPPurchased Energy (Agriculture, Food and Forest Products)CDPCDPCDPCDPCDPAverage Fleet Fuel (Transportation)Building(Materials and Buildings)Non-mechanical Emissions  \\n(Agriculture, Food and Forest Products)Percentage of Water (Energy)CDP-SASB, SASB-CDPInvestment (Agriculture, Food and Forest Products)CDP-GRI, GRI-CDPCDPCDPCDPCDPCDPRevenues (Transportation)Water Intensity (Materials and Buildings)Land Use(Agriculture, Food and Forest Products)Investments(Energy)SASB-CDPPercentage Carbon (Financial Services)SASB-CDPCDPCDPCDPCDPCDP  Full   Reasonable   Moderate   Very limited   NoneLegend Œ Alignment to TCFD     Legend Œ Mapping between frameworksAlignment with the TCFD illustrative example metrics, and between CDP, GRI and SASB\\n\\n\\n is not applicable for reporting with CDP™s framework. \\nThe name of each of the 50 TCFD illustrative example metrics is given in bold with the applicable sectors indicated in brackets.APPENDIX 4: METRICS RECOMMENDED IN THE TCFD \\nGUIDANCE AND POSSIBLE SOURCES\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 56\\nTitle of guide to go here\\n 56\\nA4S Essential Guide to Valuations and Climate Change\\n 56\\nAPPENDIX 5\\nCASE STUDIES\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nŁ \\nŁ \\nŁ \\nŁ \\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 57\\nCASE STUDIES\\n\\nNOTICE TO READER\\n\\n\\n\\n\\nRailCo Inc.\\nWYZ Transmission\\nCandor Energy\\nForestry Plantation Asset\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 58\\nTitle of guide to go here\\n 58\\nA4S Essential Guide to Valuations and Climate Change\\n 58\\nA POTENTIAL NEW INVESTMENT\\nŁ RailCo is a leading provider of maintenance-of-way (MOW) equipment, aftermarket parts and services to the \\nrailroad industry in North America. The company™s equipment is used in the construction, maintenance and \\n\\nrepair of railroad tracks throughout North America.\\nŁ RailCo is also the largest independent provider of aftermarket services to the MOW equipment sector. MOW \\n\\n\\n\\nuseful life.\\nŁ RailCo primarily serves the needs of Class 1 railroads\\n1 and enjoys long-term relationships with its major Class \\n1 rail customers. Class 1 railroads tend to prioritize MOW expenditures highest among their capex budgets Œ \\n\\nahead of track expansion and rolling stock investment.\\nCASE STUDY Œ RAILCO INC.\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 59\\nA4S Essential Guide to Valuations and Climate Change\\n 59\\nA4S Essential Guide to Valuations and Climate Change\\n 59\\nFilter\\nAssess\\nIdentify\\nIntegrate\\nIDENTIFY\\nRailCo\\nIdentify the company™s value drivers\\nMOW spend Œ driven by:\\nLong-standing relationships with Class 1 railroads\\nGross ton miles\\nŁ \\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1Track capacity\\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\nWorker safety / productivity\\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nhttps://www.aar.org/wp-content/uploads/2020/07/AAR-Coal-Fact-Sheet.pdf\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 60\\nA4S Essential Guide to Valuations and Climate Change\\n 60\\nA4S Essential Guide to Valuations and Climate Change\\n 60\\nASSESS\\nAssess climate change risk and opportunities\\n\\nDiscussions with management\\n\\n1. \\nOpportunity: \\n\\n2.\\n Opportunity:\\n\\n3.\\n Risk:\\n\\nSubject company annual and sustainability reporting\\n\\n\\nComparable company annual and sustainability reporting\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 61\\nA4S Essential Guide to Valuations and Climate Change\\n 61\\nA4S Essential Guide to Valuations and Climate Change\\n 61\\nASSESS\\nAssess climate change risks and opportunities\\n\\nSASB materiality map\\nŁ \\n\\nŁ \\n\\n\\nEquity analyst report\\n\\n\\n\\n\\n\\nExternal data providers\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 62\\nA4S Essential Guide to Valuations and Climate Change\\n 62\\nA4S Essential Guide to Valuations and Climate Change\\n 62\\nASSESS\\nAssess climate change risks and opportunities\\n\\n\\n\\n\\nRisks and Opportunities\\nManagement\\nSubject \\ncompany \\n\\nreporting\\n1Comparable \\n\\ncompany \\n\\nreporting\\n2SASB materiality \\n\\nmap\\nEquity analyst \\n\\nreports\\nExternal data \\n\\nproviders\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 63\\nA4S Essential Guide to Valuations and Climate Change\\n 63\\nA4S Essential Guide to Valuations and Climate Change\\n 63\\nASSESS\\nPhysical risks and opportunities Œ background\\nChronic opportunity: Rising temperatures\\nŁ \\nŁ \\n\\nŁ \\n\\n\\nŁ \\n\\nAcute opportunity: Extreme weather events\\nŁ \\nŁ \\n\\n\\n\\nŁ \\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 64\\nA4S Essential Guide to Valuations and Climate Change\\n 64\\nA4S Essential Guide to Valuations and Climate Change\\n 64\\nASSESS\\nTransition risks and opportunities Œ background\\nMarket risk: Class 1 exposure to coal\\nŁ \\n\\n\\n\\n1Ł \\n2Ł \\n\\n\\nŁ \\n\\n\\n\\n\\n\\nhttps://www.aar.org/wp-content/uploads/2020/07/AAR-Coal-Fact-Sheet.pdf\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 65\\nA4S Essential Guide to Valuations and Climate Change\\n 65\\nA4S Essential Guide to Valuations and Climate Change\\n 65\\nFILTER\\n\\nPhysical risks:\\nRisk / opportunity\\nMitigating considerations\\nLikelihood\\nImpact\\nChronic opportunity\\nRising temperatures: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAcute opportunity\\nExtreme weather events:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n High\\n\\nIn general, physical climate change risks present opportunities for RailCo through the potential need for more \\n\\nMOW equipment and repair / maintenance activities.\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 66\\nA4S Essential Guide to Valuations and Climate Change\\n 66\\nA4S Essential Guide to Valuations and Climate Change\\n 66\\nFILTER\\n\\nPhysical risks\\nRisk / opportunity\\nMitigating considerations\\nLikelihood\\nImpact\\nMarket risk\\nRailroad exposure to coal volumes: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n High\\nThe trend away from the use of coal for electric power generation is driven by the need to reduce GHG emissions. \\n\\nThis trend plus improvements in technology (eg fracking) and cost reductions for renewable / alternative energy \\n\\nsources increases the market risk assessment.\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 67\\nA4S Essential Guide to Valuations and Climate Change\\nIntegrate\\n 67\\nA4S Essential Guide to Valuations and Climate Change\\n 67\\nINTEGRATE\\nIntegrate climate-risk assessment into the valuation\\nVALUATION METHODOLOGY\\n\\n\\n\\n\\n\\nSecondary valuation approach: Comparable-company market approach\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 68\\nA4S Essential Guide to Valuations and Climate Change\\n 68\\nA4S Essential Guide to Valuations and Climate Change\\n 68\\nINTEGRATE\\n\\n\\n\\n\\n\\nPhysical risks:\\nRisk / opportunity\\nRisk rating\\n\\nConclusion\\nPhysical chronic opportunity\\n\\n\\n\\nŁ \\n\\nDiscount rate impacts\\nŁ \\n\\n\\n\\n\\nConsider in \\n\\ndiscount rate\\nPhysical acute opportunity\\n\\n High\\n\\nŁ \\n\\nDiscount rate impacts\\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\n\\nConsider in \\n\\ndiscount rate\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 69\\nA4S Essential Guide to Valuations and Climate Change\\n 69\\nA4S Essential Guide to Valuations and Climate Change\\n 69\\nINTEGRATE\\n\\n\\n\\n\\n\\nPhysical risks:\\nRisk / opportunity\\nRisk rating\\n\\nConclusion\\nTransition market risk\\n\\n\\n\\nŁ \\n\\n\\n\\n\\nDiscount rate impacts\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConsider in \\n\\nboth discount \\n\\nrate and cash \\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 70\\nA4S Essential Guide to Valuations and Climate Change\\n 70\\nA4S Essential Guide to Valuations and Climate Change\\n 70\\nINTEGRATE\\nComparable-company market approach\\n\\n\\n\\n\\nCompany\\nSector\\nGeography\\nPhysical climate assessment\\nTransition climate \\nassessment\\nRailCo\\n\\n\\n High\\n\\nT Corp\\n\\n\\n High\\n\\nRailCo and T Corp have similar sector and geographic exposure. Neither company has published any industry-\\n\\n\\n\\nhigher transition risk because of the Class 1 railroad exposure to declining coal volumes.\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 71\\nA4S Essential Guide to Valuations and Climate Change\\n 71\\nA4S Essential Guide to Valuations and Climate Change\\n 71\\nINTEGRATE\\nComparable-company market approach\\n\\nDescription\\nEnterprise Value / \\nEBITDA Multiple\\nComments\\nComparable-company: T Corp\\n\\n\\nAdjustments:\\nŁ \\n\\n\\n\\n\\n\\nŁ \\n\\n\\nŁ \\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\nSubject company selected multiple\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 72\\nA4S Essential Guide to Valuations and Climate Change\\n 72\\nA4S Essential Guide to Valuations and Climate Change\\n 72\\nTRIANGULATE\\nSummary of lessons learned\\nŁ \\n\\nŁ \\n\\n\\n\\nŁ \\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 73\\nTitle of guide to go here\\n 73\\nA4S Essential Guide to Valuations and Climate Change\\n 73\\nINVESTMENT OVERVIEW\\nWYZ Transmission has a long history in the \\nelectricity transmission sector.\\nActivities include:\\n\\nŁ Transmission of electricity\\nŁ Connection of power plants to the grid\\nWYZ operates in both the regulated and \\nunregulated sectors:\\nŁ Regulated: grid operations\\nŁ Unregulated: other activities such as the \\nconnection of new renewable energy power \\n\\nplants to the grid\\nPower generation\\nTransmission\\nConnection \\nof renewable \\nenergy \\nplants to the \\ngrid\\nDistribution\\nCommercial, \\nindustrial and \\nresidential \\nconsumers\\nCASE STUDY Œ WYZ TRANSMISSION\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 74\\nA4S Essential Guide to Valuations and Climate Change\\n 74\\nA4S Essential Guide to Valuations and Climate Change\\n 74\\nIDENTIFY\\nIdentify the company™s value drivers\\nCore activities\\nŁ \\n\\nRevenue / costs\\nRegulated activities\\nŁ Customers: \\n\\nŁ Revenue Source:\\n\\nŁ \\n\\nUnregulated activities\\nŁ Customers: \\n\\nŁ Revenue source:\\n\\n\\nŁ \\n\\nOperating environment\\nŁ \\n\\nŁ \\n\\n\\n\\nGeographies\\nŁ \\n\\nA4S Essential Guide to Valuations and Climate Change\\n 75\\nA4S Essential Guide to Valuations and Climate Change\\n 75\\nA4S Essential Guide to Valuations and Climate Change\\n 75\\nASSESS\\nAssess\\n\\n\\n\\n\\n\\nPhysical risks\\nType\\nRisk\\nMitigants\\nLikelihood\\nImpact\\nAcute\\n\\n\\ndamage infrastructure and \\ndisrupt activities\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 76\\nA4S Essential Guide to Valuations and Climate Change\\n 76\\nA4S Essential Guide to Valuations and Climate Change\\n 76\\nASSESS\\n\\n\\n\\n\\n\\nTransition risks\\nType\\nRisk\\nMitigants\\nLikelihood\\nImpact\\nPolicy\\nEarly closure of traditional power plants \\nbecause of policy changes\\nExample:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMarket\\nReduction in demand for energy through \\n\\ntransmission network\\nExample:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIncreased supply uncertainty from \\n\\nreplacement of ageing thermal plants\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n High\\n High\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 77\\nA4S Essential Guide to Valuations and Climate Change\\n 77\\nA4S Essential Guide to Valuations and Climate Change\\n 77\\nASSESS\\n\\nTransition risks:\\nType\\nRisk\\nMitigants\\nLikelihood\\nImpact\\nTechnology\\nDisruptive factor: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n High\\n\\nLegal and \\n\\nReputation\\nLegal action / damage to reputation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 78\\nA4S Essential Guide to Valuations and Climate Change\\n 78\\nA4S Essential Guide to Valuations and Climate Change\\n 78\\nASSESS\\n\\nOpportunities:\\nType\\nOpportunities\\nEnablers\\nLikelihood\\nImpact\\nEnergy \\nsource\\nEmergence of new renewable energy power \\n\\nplants\\n\\n\\n\\n\\n\\n\\n\\n High\\n High\\nMarket\\nEmergence of off-grid living\\n\\n\\n\\n\\n\\n High\\n\\nResource \\n\\n\\n\\n\\n\\n\\nExample:\\n\\n\\n\\n High\\n High\\nProducts and \\n\\nservices\\nPosition company as an industry leader in \\n\\nenergy transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 79\\nA4S Essential Guide to Valuations and Climate Change\\n 79\\nA4S Essential Guide to Valuations and Climate Change\\n 79\\nFILTER\\nDetermine materiality, likelihood and ability to quantify the risk\\n\\nopportunity related to the emergence of new renewable energy power plants\\n\\n\\nData limitations:\\n\\n\\n\\nOpportunity\\nCurrent risk \\nrating\\n\\nDiscount rate\\n\\n\\nconclusion\\nEmergence of new \\n\\nrenewable energy power \\n\\nplants\\n High\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 80\\nA4S Essential Guide to Valuations and Climate Change\\n 80\\nA4S Essential Guide to Valuations and Climate Change\\n 80\\nINTEGRATE\\n\\n\\n\\n\\n\\n increase fair market value\\nIncreased discount rate =  \\ndecrease fair market value\\nStep 1:\\nAdjustments to \\nCASH FLOWS\\n:\\n\\n\\nStep 2:\\nAdjustments are made to the \\nDISCOUNT RATE\\n:\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 81\\nA4S Essential Guide to Valuations and Climate Change\\n 81\\nWYZ Transmission\\nA4S Essential Guide to Valuations and Climate Change\\n 81\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\nINTEGRATE\\nWhat this means in the valuation calculation?\\n\\n\\n\\nStep 1\\n\\nŁ \\nŁ \\nStep 2\\nDetermine appropriate discount rate based on projections risk:\\n\\nŁ \\n\\nStep 1\\n\\nŁ \\nŁ \\nŁ \\nStep 2\\nDetermine appropriate discount rate based on projections risk:\\n\\nŁ \\n\\nRegulated activities\\nUnregulated activities\\nDiscount rate used for \\nregulated activities\\n*\\n10.0%\\nDiscount rate used for \\n\\nunregulated activities\\n*\\n12.0%\\n\\n\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nŁ \\nŁ \\nŁ \\nŁ \\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 82\\nA4S Essential Guide to Valuations and Climate Change\\n 82\\nA4S Essential Guide to Valuations and Climate Change\\n 82\\nTRIANGULATE\\nConclusions and lessons learned\\n\\nimportant to always keep in mind what market \\nparticipants consider\\nŁ \\nthey are not currently priced \\n\\ninto market transactions\\n\\n\\n\\npositive impact on fair market value\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 83\\nTitle of guide to go here\\n 83\\nA4S Essential Guide to Valuations and Climate Change\\n 83\\nINVESTMENT OVERVIEW\\nŁ Large portfolio of real estate assets acquired in India\\nŁ \\nŁ \\nŁ Investment thesis: to maximize income through portfolio lease-up and completion of construction\\nŁ \\nCASE STUDY Œ CANDOR ENERGY\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 84\\nA4S Essential Guide to Valuations and Climate Change\\n 84\\nA4S Essential Guide to Valuations and Climate Change\\n 84\\nIDENTIFY\\nIdentify the company™s value drivers\\nCore activities\\nŁ \\nŁ \\nRevenue\\nŁ \\nŁ \\n\\nGeographies\\nŁ India\\nCost\\nŁ \\n\\n\\nŁ \\nŁ \\n\\n\\nŁ \\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 85\\nA4S Essential Guide to Valuations and Climate Change\\n 85\\nA4S Essential Guide to Valuations and Climate Change\\n 85\\nASSESS Œ RISKS AND OPPORTUNITIES\\nIdentify the risks and opportunities\\n\\n\\nŁ \\n\\n \\n\\nŁ \\n\\n\\n \\nŁ \\nŁ \\n\\nŁ \\n\\n\\nINITIATIVES\\nŁ Electricity from the grid replaced DGs\\nŁ Substation and transmission upgrades\\nŁ Rooftop solar installations\\nŁ HVAC system re-engineered\\nŁ LED lights installed\\nŁ \\nŁ Conventional fans replaced with electronically commuted fans\\nŁ \\nRESULTS\\nŁ Over US$3.6 million in annual energy-cost savings\\nŁ Payback period of 10 months\\nŁ CO2 savings of over 16,000 tons annually\\nŁ \\nŁ \\n\\nŁ Compliance with expected legislation\\nŁ Limited potential negative impacts on tenant operations\\nŁ \\n\\nA4S Essential Guide to Valuations and Climate Change\\n 86\\nA4S Essential Guide to Valuations and Climate Change\\n 86\\nA4S Essential Guide to Valuations and Climate Change\\n 86\\nFILTER\\n\\n\\nRisks\\nRating\\nMitigating Factors\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOVERALL PHYSICAL RISK ASSESSMENT\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOVERALL TRANSITIONAL RISK ASSESSMENT\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 87\\nA4S Essential Guide to Valuations and Climate Change\\n 87\\nA4S Essential Guide to Valuations and Climate Change\\n 87\\nFILTER\\n\\n\\nOpportunities\\nImpact\\n\\n High\\n\\n High\\n\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 88\\nA4S Essential Guide to Valuations and Climate Change\\n 88\\nA4S Essential Guide to Valuations and Climate Change\\n 88\\nINTEGRATE\\n\\n\\nRisks\\nImpact on fair \\nvalue\\n\\n\\n\\n\\nAdjustments made to the discount rate\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 89\\nA4S Essential Guide to Valuations and Climate Change\\n 89\\nA4S Essential Guide to Valuations and Climate Change\\n 89\\nINTEGRATE\\nWhat does this mean for the valuation?\\n\\nCleaner energy sources\\n14%\\n<13%\\nCapitalization Rate\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 90\\nA4S Essential Guide to Valuations and Climate Change\\n 90\\nA4S Essential Guide to Valuations and Climate Change\\n 90\\nTRIANGULATE\\nTriangulate\\nLessons learned\\n\\n\\n\\nin future valuation periods.\\nFurther considerations and lessons learned:\\nŁ \\n\\nŁ \\nŁ \\n\\nA4S Essential Guide to Valuations and Climate Change\\n 91\\nTitle of guide to go here\\n 91\\nA4S Essential Guide to Valuations and Climate Change\\n 91\\nA forestry plantation asset, including a \\nlarge estate of radiata pine on freehold \\n\\nland, a nursery and native forest\\nCASE STUDY Œ FORESTRY PLANTATION ASSET\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 92\\nA4S Essential Guide to Valuations and Climate Change\\n 92\\nA4S Essential Guide to Valuations and Climate Change\\n 92\\nIDENTIFY\\nIdentify the company™s value drivers\\nCore activities\\nŁ \\nŁ \\n\\nŁ \\nRevenue / costs\\nŁ \\n\\nŁ \\n1\\n\\nGeographies\\nŁ \\nŁ \\n\\n\\n\\nŁ \\nOperating environment\\nŁ \\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 93\\nA4S Essential Guide to Valuations and Climate Change\\n 93\\nA4S Essential Guide to Valuations and Climate Change\\n 93\\nASSESS\\nAssess climate change risks and mitigating factors\\nPolicy and legal\\nTechnology\\nMarket\\nReputation\\nAcute physical\\nChronic physical\\nRisks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\nŁ \\n\\nŁ \\nMitigants \\n\\n\\n\\n\\n\\nŁ \\n\\n\\nŁ Management is \\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n1\\nŁ \\n\\n\\n\\nŁ \\nŁ \\nŁ \\n\\n\\nA4S Essential Guide to Valuations and Climate Change\\n 94\\nA4S Essential Guide to Valuations and Climate Change\\n 94\\nA4S Essential Guide to Valuations and Climate Change\\n 94\\nASSESS\\nAssess climate change opportunities\\n\\nEnergy source\\nProducts and services\\nMarkets\\nResilience\\nOpportunities\\n\\nlengthen the growing season\\n \\n\\n\\n\\n\\n\\ncarbon \\n\\nsequestration\\nIncreased demand\\n\\n\\n\\n\\nEnablers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA4S Essential Guide to Valuations and Climate Change \\n95\\nA4S Essential Guide to Valuations and Climate Change \\n95\\nA4S Essential Guide to Valuations and Climate Change \\n95\\nFILTER\\nDetermine materiality, likelihood and ability to quantify the risk\\nRisk\\nImportant according to\\nSASB materiality map?\\n Materiality\\nLikelihood\\nImpact on valuation\\nCan be \\n\\n\\nTransaction data \\n\\navailable?\\n\\n\\n\\n\\nre-nationalization \\n\\nof water rights \\n\\n\\n\\n\\n\\n\\nNo\\n\\n Low\\n\\n\\n\\n\\n\\n\\n High\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYes\\n\\n\\nNo\\n\\n\\n\\nthe \\n\\n.Yes\\n\\n\\n\\n Moderate / high\\n\\n\\n\\n\\n\\n\\n\\n High\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYes\\n\\n\\n\\n\\nNo\\n\\n\\n\\n\\n\\n\\nprevalence \\nof droughts\\nYes\\n\\n\\n\\n\\n Low\\n\\n\\n\\n\\n\\n High\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNo\\nnot enough \\n\\n\\n\\n\\nNoTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nŁ \\nŁ \\nŁ \\nŁ \\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\nForestry Plantation\\n\\nA4S Essential Guide to Valuations and Climate Change   \\n96\\nA4S Essential Guide to Valuations and Climate Change\\n96\\nA4S Essential Guide to Valuations and Climate Change\\n96\\nFILTER\\nDetermine materiality, likelihood and ability to quantify the risk\\nRisk\\nImportant according to \\nSASB materiality map?Materiality\\nLikelihood\\nImpact on valuation\\nCan be \\n\\n\\nTransaction data \\n\\navailable?\\n\\n\\n\\n\\n\\n\\n\\nYes\\n\\n\\n\\n Moderate / high\\n\\n\\n\\n\\n\\n\\n\\nand management \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Moderate\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ndisease management \\n\\n\\n\\n\\n\\n\\n\\n\\nYes\\n\\n\\nNoForestry Plantation\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\nIntroduction\\nAuthority and scope of \\nthe framework\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nŁ\\nŁ\\nŁ\\nŁ\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change   \\n97\\nA4S Essential Guide to Valuations and Climate Change\\n97\\nA4S Essential Guide to Valuations and Climate Change\\n97\\nFILTER\\nDetermine materiality, likelihood and ability to quantify the risk\\nRisk\\nImportant according to \\nSASB materiality map?Materiality\\nLikelihood\\nImpact on valuation\\nCan be \\n\\n\\nTransaction data \\n\\navailable?\\n\\n\\n\\n\\n\\n\\n\\ncarbon \\n\\nsequestration\\nNo\\n\\n Low\\n\\n\\n\\n\\n\\n\\n Moderate\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNo\\nnot enough \\n\\n\\n\\nthis stage and \\n\\n\\nNoIncreased demand\\n\\nfor timber is\\n\\n\\n\\n\\n\\n\\n\\n No\\n\\n Moderate\\n\\n\\n\\n\\n\\n\\n\\n Moderate\\n\\n\\n\\n\\n\\n\\n\\nNo\\nnot enough \\n\\n\\n\\n\\nNo\\n\\n\\nlengthen the \\n\\ngrowing season\\n\\n\\n\\n\\nYes\\n\\n\\n\\n\\n Low\\n\\n\\n\\n\\n\\n\\n\\n\\n High\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNo\\nnot enough \\n\\n\\n\\n\\nNoIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nŁ \\nŁ \\nŁ \\nŁ \\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\n\\nA4S Essential Guide to Valuations and Climate Change   \\n98\\nA4S Essentia\\nl Guide to Valuations and Climate Change\\n98\\nA4S Essential Guide to Valuations and Climate Change\\n98\\nINTEGRATE\\n\\n*Description\\nLow case\\nBase case\\nHigh case\\nGlobal temperature rise\\n\\n\\n\\n\\n\\n\\n\\n\\nCarbon price\\n\\n\\n\\nFossil fuel usage\\n\\n\\n\\n\\nfrom higher temperatures \\nand droughts\\nLOW\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlow\\nMODERATE\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nmoderate\\nHIGH\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nhigh\\n\\n\\nmost likely to play out based on the information \\n\\ncurrently available.\\nhttps://www.tcfdhub.org/scenario-analysis\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nŁ \\nŁ \\nŁ \\nŁ \\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change   \\n99\\nA4S Essential Guide to Valuations and Climate Change\\n99\\nA4S Essential Guide to Valuations and Climate Change\\n99\\nINTEGRATE\\nValuation challenges and learnings Œ What have we learned in the process?\\nDevelop partnership with leading catastrophe\\ninsurance actuary and monitor developments \\n\\n\\n\\nmodelling to better understand this risk.\\n \\n\\n\\n\\n\\nEngage with the asset manager to ensure \\n\\nthe relevant information needed to make \\n\\ninvestment decisions and forecast harvest\\n\\nyields is understood.\\n \\n\\n\\n\\n\\n\\n\\nTest data with the asset manager and make \\n\\nadjustments based on their observations and\\n\\npractical experience.\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nData \\nlimitations\\nChallenges\\nSolutions and learnings\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nŁ \\nŁ \\nŁ \\nŁ \\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\nForestry Plantation\\n\\nA4S Essential Guide to Valuations and Climate Change   \\n100\\nA4S Essential Guide to Valuations and Climate Change\\n100\\nA4S Essential Guide to Valuations and Climate Change\\n100\\nTRIANGULATE\\nTest adjustments and monitor company strategy around climate change\\nWhat factors will we monitor?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat factors will we monitor?\\nProactively engage with asset manager to:\\nDevelop a climate change framework\\n\\n\\n\\n\\n\\nBetter understand asset climate change\\nrisks and opportunities\\n \\n\\n\\n\\nTriangulate\\nIntegrate\\nFilter\\nAssess\\nIdentify\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nŁ \\nŁ \\nŁ \\nŁ \\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\nForestry Plantation\\n\\nA4S Essential Guide to Valuations and Climate Change  \\n101\\nA4S Essential Guide to Va\\nluations and Climate Change\\n101\\nANNEX - APPLICATION OF THE LENSES\\nTO FORESTRY PLANTATION\\n  Policy and legal\\nŁ \\n\\n\\n\\n\\n\\ndesigned to limit \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTechnology\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\nMarket\\nŁ Changes in \\n\\n\\n\\n\\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\nReputation\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAcute\\nŁ \\n\\n\\n\\n\\n\\n\\n\\nChronic\\nŁ \\n\\n\\n\\n\\n\\n\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nŁ \\nŁ \\nŁ \\nŁ \\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nA4S Essential Guide to Valuations and Climate Change \\n102\\nA4S Essential Guide to Valuations and Climate Change \\n102\\nANNEX - APPLICATION OF THE LENSES \\n TO FORESTRY PLANTATION\\nResource \\n\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnergy\\nsource\\n Ł \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts /\\nservices\\n Ł \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMarkets\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResilience\\nŁ \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nŁ \\nŁ \\nŁ \\nŁ \\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\nForestry Plantation\\n\\nA4S Essential Guide to Valuations and Climate Change \\n103\\nTHE A4S CFO LEADERSHIP NETWORK\\nNETWORK MEMBERS Œ EUROPE\\nClifford Abrahams\\n\\nSteve Buck\\n\\nDavid Walker\\n\\nJulie Brown\\n\\nPhilippe Blondiaux\\n, Chanel\\nMelanie Kreis\\n\\nTim Harris\\n\\nIain Mackay\\n\\nJavier Echave\\n\\nHoldings\\nAndy Agg\\n\\nPaul Boote\\n\\nHenry Schirmer\\n, Randstad\\nGeraldine Matchett\\n\\nMaria Ferraro\\n\\nGregor Alexander\\n, SSE\\nSeppo Parvi\\n\\nLars Løddesøl\\n\\nAlan Stewart\\n\\nKate Bowyer\\n\\nGraeme Pitkethly\\n\\nChris Johns\\n\\nGeorge Quinn\\n\\nThe Prince™s Accounting for Sustainability \\nProject (A4S)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nwww.accountingforsustaina\\nbility.org\\nOur project team would value feedback on \\n\\nthis guide from other\\n organizations working\\nin this area. Please send any comments to:\\n info@a4s.org\\n NETWORK MEMBERS Œ CANADA\\nLawrence Davis\\n\\nBrian Lawson\\n\\nManagement\\nMaarika Paul\\n\\n\\nPatrice Impey\\n\\nJocelyn Perry\\n\\nPhilip Witherington\\n\\n\\nJonathan Simmons\\n\\n\\nDavid McGraw\\n\\n\\nPension Plan\\nPamela Steer\\n\\nDoug French\\n, TELUS\\nKaren Higgins\\n\\nVictor Pang\\n\\n\\nNETWORK MEMBERS Œ US\\nRay Young\\n\\n\\n\\nScott Herren,\\n\\nKeith Taylor\\n\\nGeorge Davis\\n, Intel\\nHarmit Singh\\n\\nClaus Aagaard\\n\\nMark Kaye\\n\\nEwout Steenbergen\\n\\nMark Hawkins\\n\\nWarren Zaccaro\\n\\n\\nMatthew Ellis\\n\\nZane Rowe\\n\\nIntroduction\\nAutorité et champ \\nd™application du cadre\\nCadre d™évaluation axé \\nsur les changements \\n\\nclimatiques\\nAnnexes 1-4\\n\\nAnnexe 5 Œ Études de cas\\nGUIDE ESSENTIEL SUR \\nLES ÉVALUATIONS ET LES \\n\\nCHANGEMENTS CLIMATIQUES\\nRÉSEAU\\n DE LEADERSHIP \\nDES CHEFS DES \\nFINANCES DE \\nL™ADC\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n\\nGET IN TOUCH OR FIND OUT MORE\\n@PrincesA4S\\nThe Prince™s Accounting for Sustainability Project (A4S)\\ninfo@a4s.org\\nwww.accountingforsustainability.org\\nThePrincesA4S\\nIntroduction\\nAutorité et champ \\nd™application du cadre\\nCadre d™évaluation axé \\nsur les changements \\n\\nclimatiques\\nAnnexes 1-4\\n\\nAnnexe 5 Œ Études de cas\\nGUIDE ESSENTIEL SUR \\nLES ÉVALUATIONS ET LES \\n\\nCHANGEMENTS CLIMATIQUES\\nRÉSEAU\\n DE LEADERSHIP \\nDES CHEFS DES \\nFINANCES DE \\nL™ADC\\nIntroduction\\nAuthority and scope of \\n the framework\\n\\nClimate change valuation \\nframework\\nAppendices 1-4\\n\\nAppendix 5 Œ Case studies\\nEssential Guide to \\nVALUATIONS AND \\n\\nCLIMATE CHANGE\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYzFED0EH1lN"
      },
      "source": [
        "esgRAWtextfile = open(r\"esgRAWtext.txt\",\"w\")\n",
        "\n",
        "esgRAWtextfile.writelines(adfreportsdf.text.to_numpy())"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXKqy3pGf82M"
      },
      "source": [
        "# ESG BERT Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EBdU9GXgCX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f0a343-a114-4ac1-9e40-90606428d4e3"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!git clone https://github.com/google-research/bert"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Cloning into 'bert'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 328.28 KiB | 4.00 MiB/s, done.\n",
            "Resolving deltas: 100% (182/182), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSvcywMHvOZK",
        "outputId": "675a1201-5383-4a92-e421-058e2c06f9f3"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install tensorflow==1.15.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/72/d06017379ad4760dc58781c765376ce4ba5dcf3c08d37032eeefbccf1c51/tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 28kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.34.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 34.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.19.5)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.36.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15.0) (57.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (4.5.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=a709811d9bb7d96d690c449a058988a80f5a03a2dd13787c96e0fc8ea45b73ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement gast==0.4.0, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorflow-estimator<2.6.0,>=2.5.0rc0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorboard, keras-applications, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.36.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.34.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (57.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "Successfully installed tensorflow-1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwbeX7sqxOMm"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyjWpUOVgUbI",
        "outputId": "beb3bc18-51ca-4bf4-cd51-0031a05f8dd6"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "sys.path.append(\"bert\")\n",
        "\n",
        "from bert import modeling, optimization, tokenization\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "auth.authenticate_user()\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setLevel(logging.INFO)\n",
        "sh.setFormatter(formatter)\n",
        "log.handlers = [sh]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  USE_TPU = True\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "  USE_TPU = False"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-06 13:02:09,539 :  Using TPU runtime\n",
            "2021-07-06 13:02:09,546 :  TPU address is grpc://10.18.117.138:8470\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RLaGfHq0OqW"
      },
      "source": [
        "def normalize_text(text):\n",
        "  # remove non-UTF\n",
        "  text = text.encode(\"utf-8\", \"ignore\").decode()\n",
        "  # remove \\n\n",
        "  text = text.replace(\"\\n\",\" \")\n",
        "  # remove extra spaces\n",
        "  text = ' '.join(text.split())\n",
        "  return text\n",
        "\n",
        "def count_lines(filename):\n",
        "  count = 0\n",
        "  with open(filename) as fi:\n",
        "    for line in fi:\n",
        "      count += 1\n",
        "  return count"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuMQavKbLY01",
        "outputId": "59fd2e9a-308a-4192-ef50-16929b2b5e1e"
      },
      "source": [
        "RAW_DATA_FPATH = \"esgRAWtext.txt\" #@param {type: \"string\"}\n",
        "PRC_DATA_FPATH = \"esgPROCESSEDtext.txt\" #@param {type: \"string\"}\n",
        "\n",
        "# apply normalization to the dataset\n",
        "# this will take a minute or two\n",
        "\n",
        "total_lines = count_lines(RAW_DATA_FPATH)\n",
        "bar = Progbar(total_lines)\n",
        "\n",
        "with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi:\n",
        "  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
        "    for l in fi:\n",
        "      fo.write(normalize_text(l)+\"\\n\")\n",
        "      bar.add(1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6686/6686 [==============================] - 0s 4us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFaSeOT4Cpsc"
      },
      "source": [
        "MODEL_PREFIX = \"tokenizer\" #@param {type: \"string\"}\n",
        "VOC_SIZE = 2000 #@param {type:\"integer\"}\n",
        "SUBSAMPLE_SIZE = 12800000 #@param {type:\"integer\"}\n",
        "NUM_PLACEHOLDERS = 256 #@param {type:\"integer\"}\n",
        "\n",
        "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
        "               '--vocab_size={} --input_sentence_size={} '\n",
        "               '--shuffle_input_sentence=true ' \n",
        "               '--bos_id=-1 --eos_id=-1').format(\n",
        "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
        "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNQ1ygUyaGAS",
        "outputId": "5adc6f4e-fc40-46b9-e6e5-42ca8425508d"
      },
      "source": [
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 1743\n",
            "Sample tokens: ['▁Company', 'just', 'tud', '▁Revenue', 'ARE', '▁private', 'based', '▁strong', '▁Water', 'Institution']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2gexbZmah6O",
        "outputId": "c66c4c6f-9041-4ed3-f230-40802015c406"
      },
      "source": [
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token\n",
        "        \n",
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab\n",
        "\n",
        "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdDqCeBsamc2"
      },
      "source": [
        "VOC_FNAME = \"esgvocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF5kfYxNa-9P",
        "outputId": "e79e4770-facf-47fc-c94e-5303a7b56c43"
      },
      "source": [
        "testcase = \"Certain sectors are particularly vulnerable to climate risk.  For example, the TCFD recommendations state that companies engaged in fossil-fuel based industries, energy-intensive manufacturing, and transportation activities are most exposed to transition risks, while those engaged in agriculture, transportation and building infrastructure, insurance and tourism are more exposed to physical risks.\"\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(testcase)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-06 13:04:46,621 :  From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['c',\n",
              " '##er',\n",
              " '##t',\n",
              " '##ain',\n",
              " 'sectors',\n",
              " 'are',\n",
              " 'particular',\n",
              " '##ly',\n",
              " 'v',\n",
              " '##u',\n",
              " '##l',\n",
              " '##ne',\n",
              " '##ra',\n",
              " '##ble',\n",
              " 'to',\n",
              " 'climate',\n",
              " 'risk',\n",
              " '[UNK]',\n",
              " 'for',\n",
              " 'example',\n",
              " '[UNK]',\n",
              " 'the',\n",
              " 't',\n",
              " '##c',\n",
              " '##f',\n",
              " '##d',\n",
              " 'reco',\n",
              " '##m',\n",
              " '##me',\n",
              " '##nd',\n",
              " '##ations',\n",
              " 'state',\n",
              " 'that',\n",
              " 'companies',\n",
              " 'en',\n",
              " '##ga',\n",
              " '##ge',\n",
              " '##d',\n",
              " 'in',\n",
              " 'f',\n",
              " '##ossil',\n",
              " '[UNK]',\n",
              " 'f',\n",
              " '##u',\n",
              " '##e',\n",
              " '##l',\n",
              " 'based',\n",
              " 'industri',\n",
              " '##es',\n",
              " '[UNK]',\n",
              " 'energy',\n",
              " '[UNK]',\n",
              " 'intensi',\n",
              " '##ve',\n",
              " 'ma',\n",
              " '##n',\n",
              " '##u',\n",
              " '##f',\n",
              " '##act',\n",
              " '##ur',\n",
              " '##ing',\n",
              " '[UNK]',\n",
              " 'and',\n",
              " 'tra',\n",
              " '##n',\n",
              " '##s',\n",
              " '##p',\n",
              " '##ort',\n",
              " '##ation',\n",
              " 'activities',\n",
              " 'are',\n",
              " 'mo',\n",
              " '##st',\n",
              " 'exp',\n",
              " '##o',\n",
              " '##se',\n",
              " '##d',\n",
              " 'to',\n",
              " 'transition',\n",
              " 'risks',\n",
              " '[UNK]',\n",
              " 'wh',\n",
              " '##il',\n",
              " '##e',\n",
              " 'th',\n",
              " '##o',\n",
              " '##se',\n",
              " 'en',\n",
              " '##ga',\n",
              " '##ge',\n",
              " '##d',\n",
              " 'in',\n",
              " 'a',\n",
              " '##gricultur',\n",
              " '##e',\n",
              " '[UNK]',\n",
              " 'tra',\n",
              " '##n',\n",
              " '##s',\n",
              " '##p',\n",
              " '##ort',\n",
              " '##ation',\n",
              " 'and',\n",
              " 'b',\n",
              " '##uild',\n",
              " '##ing',\n",
              " 'infrastructure',\n",
              " '[UNK]',\n",
              " 'insurance',\n",
              " 'and',\n",
              " 'to',\n",
              " '##ur',\n",
              " '##is',\n",
              " '##m',\n",
              " 'are',\n",
              " 'more',\n",
              " 'exp',\n",
              " '##o',\n",
              " '##se',\n",
              " '##d',\n",
              " 'to',\n",
              " 'physical',\n",
              " 'risks',\n",
              " '[UNK]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWg1v-YpoGGa"
      },
      "source": [
        "MAX_SEQ_LENGTH = 512 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "MAX_PREDICTIONS = 75 #@param {type:\"integer\"}\n",
        "DO_LOWER_CASE = False #@param {type:\"boolean\"}\n",
        "\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
        "# controls how many parallel processes xargs can create\n",
        "PROCESSES = 2 #@param {type:\"integer\"}"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q20BPzX2Yhv"
      },
      "source": [
        "!mkdir ./shards\n",
        "!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAJPgY5vo7xE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8f9049-cd80-4238-9bbd-44db45895d99"
      },
      "source": [
        "XARGS_CMD = (\"ls ./shards/ |\"\n",
        "             \"xargs -n 1 -P {} -I{} \"\n",
        "             \"python3 bert/create_pretraining_data.py \"\n",
        "             \"--input_file=./shards/{} \"\n",
        "             \"--output_file={}/{}.tfrecord \"\n",
        "             \"--vocab_file={} \"\n",
        "             \"--do_lower_case={} \"\n",
        "             \"--max_predictions_per_seq={} \"\n",
        "             \"--max_seq_length={} \"\n",
        "             \"--masked_lm_prob={} \"\n",
        "             \"--random_seed=34 \"\n",
        "             \"--dupe_factor=5\")\n",
        "\n",
        "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
        "                             VOC_FNAME, DO_LOWER_CASE, \n",
        "                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)\n",
        "                             \n",
        "tf.gfile.MkDir(PRETRAINING_DIR)\n",
        "!$XARGS_CMD"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0706 13:05:00.541477 140220668987264 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0706 13:05:00.541745 140220668987264 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0706 13:05:00.542016 140220668987264 module_wrapper.py:139] From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0706 13:05:00.549776 140220668987264 module_wrapper.py:139] From bert/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0706 13:05:00.550603 140220668987264 module_wrapper.py:139] From bert/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:*** Reading from input files ***\n",
            "I0706 13:05:00.550797 140220668987264 create_pretraining_data.py:446] *** Reading from input files ***\n",
            "INFO:tensorflow:  ./shards/shard_0000\n",
            "I0706 13:05:00.550927 140220668987264 create_pretraining_data.py:448]   ./shards/shard_0000\n",
            "INFO:tensorflow:*** Writing to output files ***\n",
            "I0706 13:05:01.526751 140220668987264 create_pretraining_data.py:457] *** Writing to output files ***\n",
            "INFO:tensorflow:  pretraining_data/shard_0000.tfrecord\n",
            "I0706 13:05:01.527031 140220668987264 create_pretraining_data.py:459]   pretraining_data/shard_0000.tfrecord\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0706 13:05:01.527289 140220668987264 module_wrapper.py:139] From bert/create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.528677 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] [MASK] ##4 [MASK] [MASK] ##l Corporate ##uid ##e to Valuations and Climate Change 9 [MASK] A ##4 ##S Essentia [MASK] G ##uid ##e to Valuations and Climate [MASK] 9 ##3 A ##4 ##S Essentia ##l G ##uid ##e to Valuations [MASK] Climate Change 9 ##3 ASSESS Assess climate change risks and m ##itigati ##ng factors Policy and leg ##al Technology Market [SEP] [MASK] Acute physical Chronic physical [MASK] [SEP]\n",
            "I0706 13:05:01.528894 140220668987264 create_pretraining_data.py:151] tokens: [CLS] [MASK] ##4 [MASK] [MASK] ##l Corporate ##uid ##e to Valuations and Climate Change 9 [MASK] A ##4 ##S Essentia [MASK] G ##uid ##e to Valuations and Climate [MASK] 9 ##3 A ##4 ##S Essentia ##l G ##uid ##e to Valuations [MASK] Climate Change 9 ##3 ASSESS Assess climate change risks and m ##itigati ##ng factors Policy and leg ##al Technology Market [SEP] [MASK] Acute physical Chronic physical [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 2 4 11 4 4 10 1243 17 8 7 21 6 15 20 103 4 12 11 13 16 4 14 17 8 7 21 6 15 4 103 104 12 11 13 16 10 14 17 8 7 21 4 15 20 103 104 76 75 44 28 68 6 715 1059 147 376 223 6 677 102 328 84 3 4 184 202 203 202 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.529201 140220668987264 create_pretraining_data.py:161] input_ids: 2 4 11 4 4 10 1243 17 8 7 21 6 15 20 103 4 12 11 13 16 4 14 17 8 7 21 6 15 4 103 104 12 11 13 16 10 14 17 8 7 21 4 15 20 103 104 76 75 44 28 68 6 715 1059 147 376 223 6 677 102 328 84 3 4 184 202 203 202 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.529488 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.529747 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 1 3 4 6 15 20 28 41 63 68 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.529885 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 1 3 4 6 15 20 28 41 63 68 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 12 13 16 14 104 10 20 6 216 105 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.530015 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 12 13 16 14 104 10 20 6 216 105 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.530155 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0706 13:05:01.530263 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.531141 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] Operating environment [SEP] [MASK] [SEP]\n",
            "I0706 13:05:01.531296 140220668987264 create_pretraining_data.py:151] tokens: [CLS] Operating environment [SEP] [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 2 534 231 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.531583 140220668987264 create_pretraining_data.py:161] input_ids: 2 534 231 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.531827 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.532075 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.532206 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.532369 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.532521 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0706 13:05:01.541558 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.542971 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] environmental risks [UNK] wh ##i ##ch [MASK] the ##n tr ##ig ##ge [MASK] [SEP] [MASK] ##erna ##l data providers Va ##ri ##o ##us [MASK] ##terna ##l [MASK] providers are begin ##ni [MASK] to summarize and disclose [SEP]\n",
            "I0706 13:05:01.543207 140220668987264 create_pretraining_data.py:151] tokens: [CLS] environmental risks [UNK] wh ##i ##ch [MASK] the ##n tr ##ig ##ge [MASK] [SEP] [MASK] ##erna ##l data providers Va ##ri ##o ##us [MASK] ##terna ##l [MASK] providers are begin ##ni [MASK] to summarize and disclose [SEP]\n",
            "INFO:tensorflow:input_ids: 2 888 68 1 227 78 140 4 18 101 1414 1489 925 4 3 4 913 10 122 377 1329 969 559 1382 4 687 10 4 377 156 1216 1445 4 7 1169 6 924 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.543709 140220668987264 create_pretraining_data.py:161] input_ids: 2 888 68 1 227 78 140 4 18 101 1414 1489 925 4 3 4 913 10 122 377 1329 969 559 1382 4 687 10 4 377 156 1216 1445 4 7 1169 6 924 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.544192 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.544531 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 7 13 15 24 27 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.544676 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 7 13 15 24 27 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 133 163 659 685 122 147 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.544800 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 133 163 659 685 122 147 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.544934 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:01.545027 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.545934 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] Ł [SEP] [MASK] price [SEP]\n",
            "I0706 13:05:01.546072 140220668987264 create_pretraining_data.py:151] tokens: [CLS] Ł [SEP] [MASK] price [SEP]\n",
            "INFO:tensorflow:input_ids: 2 5 3 4 333 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.546345 140220668987264 create_pretraining_data.py:161] input_ids: 2 5 3 4 333 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.546589 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.546906 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.547075 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 375 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.547216 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 375 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.547370 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:01.644185 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.645745 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] regulator ##s [UNK] the government and the po ##p [MASK] that WWM serves [UNK] among [SEP] t ##wo years [UNK] Further [UNK] give ##n [MASK] tornado wa [MASK] a natur ##al dis [MASK] ##ter co ##ver ##ed by [UNK] [SEP]\n",
            "I0706 13:05:01.645994 140220668987264 create_pretraining_data.py:151] tokens: [CLS] regulator ##s [UNK] the government and the po ##p [MASK] that WWM serves [UNK] among [SEP] t ##wo years [UNK] Further [UNK] give ##n [MASK] tornado wa [MASK] a natur ##al dis [MASK] ##ter co ##ver ##ed by [UNK] [SEP]\n",
            "INFO:tensorflow:input_ids: 2 314 19 1 18 430 6 18 1437 527 4 73 153 1157 1 633 3 137 1441 805 1 531 1 509 101 4 803 267 4 33 850 102 487 4 725 555 458 59 112 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.646361 140220668987264 create_pretraining_data.py:161] input_ids: 2 314 19 1 18 430 6 18 1437 527 4 73 153 1157 1 633 3 137 1441 805 1 531 1 509 101 4 803 267 4 33 850 102 487 4 725 555 458 59 112 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.646642 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.646888 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 3 10 19 25 28 33 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.647024 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 3 10 19 25 28 33 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 1 1168 805 18 19 981 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.647157 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 1 1168 805 18 19 981 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.647311 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:01.647410 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.648316 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] [MASK] ##i ##ques [SEP] Ł Ł Ł Ł Essentia ##l G ##uid ##e to VALUATIONS AND [SEP]\n",
            "I0706 13:05:01.648463 140220668987264 create_pretraining_data.py:151] tokens: [CLS] [MASK] ##i ##ques [SEP] Ł Ł Ł Ł Essentia ##l G ##uid ##e to VALUATIONS AND [SEP]\n",
            "INFO:tensorflow:input_ids: 2 4 78 421 3 5 5 5 5 16 10 14 17 8 7 60 42 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.648743 140220668987264 create_pretraining_data.py:161] input_ids: 2 4 78 421 3 5 5 5 5 16 10 14 17 8 7 60 42 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.648993 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.649240 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 1 6 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.649389 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 1 6 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 590 5 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.649537 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 590 5 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.649705 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:01.749376 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.752066 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] A ##4 ##S Essentia ##l G ##uid ##e to Valuations and 65 Change [SEP] 5 ##6 Ti ##tle of guid ##e to go here 5 ##6 A PRO [MASK] Essentia ##l G [MASK] [UNUSED_160] to [MASK] and Climate Change 5 ##6 APPENDI ##X 5 CAS ##E STUD ##IE ##S Int ##roduction [MASK] ##h [MASK] ##y and sco ##pe of the framework [SEP]\n",
            "I0706 13:05:01.752333 140220668987264 create_pretraining_data.py:151] tokens: [CLS] A ##4 ##S Essentia ##l G ##uid ##e to Valuations and 65 Change [SEP] 5 ##6 Ti ##tle of guid ##e to go here 5 ##6 A PRO [MASK] Essentia ##l G [MASK] [UNUSED_160] to [MASK] and Climate Change 5 ##6 APPENDI ##X 5 CAS ##E STUD ##IE ##S Int ##roduction [MASK] ##h [MASK] ##y and sco ##pe of the framework [SEP]\n",
            "INFO:tensorflow:input_ids: 2 12 11 13 16 10 14 17 8 7 21 6 719 20 3 34 194 212 201 23 141 8 7 165 172 34 194 12 832 4 16 10 14 4 1908 7 4 6 15 20 34 194 169 162 34 171 99 1068 984 13 63 51 4 465 4 30 6 57 52 23 18 26 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.752840 140220668987264 create_pretraining_data.py:161] input_ids: 2 12 11 13 16 10 14 17 8 7 21 6 719 20 3 34 194 212 201 23 141 8 7 165 172 34 194 12 832 4 16 10 14 4 1908 7 4 6 15 20 34 194 169 162 34 171 99 1068 984 13 63 51 4 465 4 30 6 57 52 23 18 26 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.753282 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.753682 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 7 12 28 29 33 34 36 52 54 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.753858 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 7 12 28 29 33 34 36 52 54 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 17 15 11 13 17 8 21 686 443 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.754153 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 17 15 11 13 17 8 21 686 443 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.754348 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0706 13:05:01.754460 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.755840 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] Appendi ##x 5 [MASK] C [MASK] studies Ł Ł [MASK] Ł Essentia ##l G ##uid ##e to [SEP] VALUATIONS AND [SEP]\n",
            "I0706 13:05:01.756006 140220668987264 create_pretraining_data.py:151] tokens: [CLS] Appendi ##x 5 [MASK] C [MASK] studies Ł Ł [MASK] Ł Essentia ##l G ##uid ##e to [SEP] VALUATIONS AND [SEP]\n",
            "INFO:tensorflow:input_ids: 2 31 41 34 4 39 4 58 5 5 4 5 16 10 14 17 8 7 3 60 42 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.756403 140220668987264 create_pretraining_data.py:161] input_ids: 2 31 41 34 4 39 4 58 5 5 4 5 16 10 14 17 8 7 3 60 42 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.756759 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.757105 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 4 6 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.757263 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 4 6 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 32 50 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.757405 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 32 50 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.757561 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0706 13:05:01.849219 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.850703 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] 12 ##11 [SEP] Ł Appendi ##ces 1 [UNK] [MASK] Appendi ##x 5 Œ C ##ase studies [MASK] [MASK] G ##uid ##e to VALUATIONS AND [SEP]\n",
            "I0706 13:05:01.850953 140220668987264 create_pretraining_data.py:151] tokens: [CLS] 12 ##11 [SEP] Ł Appendi ##ces 1 [UNK] [MASK] Appendi ##x 5 Œ C ##ase studies [MASK] [MASK] G ##uid ##e to VALUATIONS AND [SEP]\n",
            "INFO:tensorflow:input_ids: 2 289 920 3 5 31 54 43 1 4 31 41 34 32 39 50 58 4 4 14 17 8 7 60 42 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.851363 140220668987264 create_pretraining_data.py:161] input_ids: 2 289 920 3 5 31 54 43 1 4 31 41 34 32 39 50 58 4 4 14 17 8 7 60 42 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.851652 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.851910 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 1 9 17 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.852043 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 1 9 17 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 289 146 16 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.852166 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 289 146 16 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.852314 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:01.852418 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.853304 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] Ł [SEP] un ##s ##ustainable ##v model [MASK] that respond ##s [SEP]\n",
            "I0706 13:05:01.853443 140220668987264 create_pretraining_data.py:151] tokens: [CLS] Ł [SEP] un ##s ##ustainable ##v model [MASK] that respond ##s [SEP]\n",
            "INFO:tensorflow:input_ids: 2 5 3 545 19 1019 1470 448 4 73 1211 19 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.853713 140220668987264 create_pretraining_data.py:161] input_ids: 2 5 3 545 19 1019 1470 448 4 73 1211 19 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.853965 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.854227 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 6 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.854381 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 6 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 177 340 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.854504 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 177 340 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.854639 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:01.954594 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.956382 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] Co [MASK] value [SEP] [UNK] Market approach [SEP]\n",
            "I0706 13:05:01.956615 140220668987264 create_pretraining_data.py:151] tokens: [CLS] Co [MASK] value [SEP] [UNK] Market approach [SEP]\n",
            "INFO:tensorflow:input_ids: 2 1272 4 70 3 1 84 217 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.957184 140220668987264 create_pretraining_data.py:161] input_ids: 2 1272 4 70 3 1 84 217 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.957689 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.958155 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.958404 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 1079 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.958588 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 1079 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.958805 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0706 13:05:01.958957 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:01.960504 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] Ł [SEP] [MASK] ##l ##ing to be [MASK] understand this risk [UNK] [SEP]\n",
            "I0706 13:05:01.960718 140220668987264 create_pretraining_data.py:151] tokens: [CLS] Ł [SEP] [MASK] ##l ##ing to be [MASK] understand this risk [UNK] [SEP]\n",
            "INFO:tensorflow:input_ids: 2 5 3 4 10 35 7 49 4 839 89 46 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.961197 140220668987264 create_pretraining_data.py:161] input_ids: 2 5 3 4 10 35 7 49 4 839 89 46 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.961668 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.962090 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 3 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.962282 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 3 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 448 456 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:01.962443 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 448 456 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:01.962617 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:02.052834 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:02.054332 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] A ##4 [MASK] Essentia ##l G ##uid ##e to Valuations and [MASK] Change 8 ##5 A ##4 ##S environmental [MASK] G ##uid ##e to Valuations and Climate Change 8 ##5 [SEP] A ##4 ##S Essentia ##l G [MASK] ##e to Valuations and Climate Change 8 ##5 ASSESS Œ RISKS AND O ##P ##PORT ##U [MASK] ##ES [MASK] the risks and [MASK] [SEP]\n",
            "I0706 13:05:02.054587 140220668987264 create_pretraining_data.py:151] tokens: [CLS] A ##4 [MASK] Essentia ##l G ##uid ##e to Valuations and [MASK] Change 8 ##5 A ##4 ##S environmental [MASK] G ##uid ##e to Valuations and Climate Change 8 ##5 [SEP] A ##4 ##S Essentia ##l G [MASK] ##e to Valuations and Climate Change 8 ##5 ASSESS Œ RISKS AND O ##P ##PORT ##U [MASK] ##ES [MASK] the risks and [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 2 12 11 4 16 10 14 17 8 7 21 6 4 20 100 335 12 11 13 888 4 14 17 8 7 21 6 15 20 100 335 3 12 11 13 16 10 14 4 8 7 21 6 15 20 100 335 76 32 378 42 704 298 362 295 4 149 4 18 68 6 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.054956 140220668987264 create_pretraining_data.py:161] input_ids: 2 12 11 4 16 10 14 17 8 7 21 6 4 20 100 335 12 11 13 888 4 14 17 8 7 21 6 15 20 100 335 3 12 11 13 16 10 14 4 8 7 21 6 15 20 100 335 76 32 378 42 704 298 362 295 4 149 4 18 68 6 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.055250 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.055516 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 3 12 19 20 28 38 55 57 61 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.055665 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 3 12 19 20 28 38 55 57 61 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 13 15 16 10 20 17 319 108 97 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.055797 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 13 15 16 10 20 17 319 108 97 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:02.056087 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0706 13:05:02.056332 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:02.057265 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] Appendi ##ces 1 [MASK] 4 [SEP] VALUATIONS AND [SEP]\n",
            "I0706 13:05:02.057470 140220668987264 create_pretraining_data.py:151] tokens: [CLS] Appendi ##ces 1 [MASK] 4 [SEP] VALUATIONS AND [SEP]\n",
            "INFO:tensorflow:input_ids: 2 31 54 43 4 146 3 60 42 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.057783 140220668987264 create_pretraining_data.py:161] input_ids: 2 31 54 43 4 146 3 60 42 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.058109 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.058411 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.058550 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 54 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.058694 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 54 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:02.058838 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:02.154284 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:02.157429 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] Re ##s ##il ##ience [SEP] premiums included [SEP]\n",
            "I0706 13:05:02.157717 140220668987264 create_pretraining_data.py:151] tokens: [CLS] Re ##s ##il ##ience [SEP] premiums included [SEP]\n",
            "INFO:tensorflow:input_ids: 2 1418 19 1301 1288 3 1337 1246 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.158206 140220668987264 create_pretraining_data.py:161] input_ids: 2 1418 19 1301 1288 3 1337 1246 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.158696 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.159130 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.159350 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 19 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.159535 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 19 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:02.159721 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:02.159822 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:02.161139 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] Risks [SEP] reporting 2 [MASK] materiality [SEP]\n",
            "I0706 13:05:02.161298 140220668987264 create_pretraining_data.py:151] tokens: [CLS] Risks [SEP] reporting 2 [MASK] materiality [SEP]\n",
            "INFO:tensorflow:input_ids: 2 105 3 186 66 4 256 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.161684 140220668987264 create_pretraining_data.py:161] input_ids: 2 105 3 186 66 4 256 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.162056 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.162434 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.162592 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 195 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.162733 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 195 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:02.162877 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:02.256649 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:02.257755 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] CHANGE ##MENT ##S CLIMAT ##I [MASK] ##ES R ##É ##SE ##AU DE [MASK] DES [MASK] ##EF ##S DES FINANC ##ES [MASK] [SEP] Energy source Ł [SEP]\n",
            "I0706 13:05:02.257913 140220668987264 create_pretraining_data.py:151] tokens: [CLS] CHANGE ##MENT ##S CLIMAT ##I [MASK] ##ES R ##É ##SE ##AU DE [MASK] DES [MASK] ##EF ##S DES FINANC ##ES [MASK] [SEP] Energy source Ł [SEP]\n",
            "INFO:tensorflow:input_ids: 2 37 261 13 604 229 4 149 79 589 563 664 193 4 338 4 382 13 338 253 149 4 3 260 270 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.258206 140220668987264 create_pretraining_data.py:161] input_ids: 2 37 261 13 604 229 4 149 79 589 563 664 193 4 338 4 382 13 338 253 149 4 3 260 270 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.258488 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.258703 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 6 13 15 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.258795 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 6 13 15 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 511 311 449 193 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.258877 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 511 311 449 193 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:02.258974 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:02.259038 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:02.259804 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] [MASK] ##ces 1 [UNK] ##ves [SEP] 5 ##8 [MASK] ##tle of guid ##e to go [MASK] 5 ##8 A ##4 ##S [MASK] ##l G ##uid ##e to Valuations and Climate [MASK] 5 ##8 A PO ##T ##ENT ##IAL N ##EW INVESTMENT Ł RailCo [MASK] a lead ##ing provider of maintenance [UNK] of [UNK] wa ##y ( [MASK] [UNK] equipment [UNK] a ##f ##term ##ar achiev [MASK] [MASK] ##s and services to the railroad industry in No ##rth A [MASK] ##rica [UNK] The [UNK] equipment is used in [MASK] construction [UNK] maintenance and [SEP]\n",
            "I0706 13:05:02.259936 140220668987264 create_pretraining_data.py:151] tokens: [CLS] [MASK] ##ces 1 [UNK] ##ves [SEP] 5 ##8 [MASK] ##tle of guid ##e to go [MASK] 5 ##8 A ##4 ##S [MASK] ##l G ##uid ##e to Valuations and Climate [MASK] 5 ##8 A PO ##T ##ENT ##IAL N ##EW INVESTMENT Ł RailCo [MASK] a lead ##ing provider of maintenance [UNK] of [UNK] wa ##y ( [MASK] [UNK] equipment [UNK] a ##f ##term ##ar achiev [MASK] [MASK] ##s and services to the railroad industry in No ##rth A [MASK] ##rica [UNK] The [UNK] equipment is used in [MASK] construction [UNK] maintenance and [SEP]\n",
            "INFO:tensorflow:input_ids: 2 4 54 43 1 1339 3 34 138 4 201 23 141 8 7 165 4 34 138 12 11 13 4 10 14 17 8 7 21 6 15 4 34 138 12 929 67 716 258 268 517 841 5 312 4 33 437 35 1041 23 798 1 23 1 267 30 45 4 1 640 1 33 381 285 997 1416 4 4 19 6 235 7 18 748 199 27 145 1275 12 4 1144 1 82 1 640 55 538 27 4 1162 1 798 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.260239 140220668987264 create_pretraining_data.py:161] input_ids: 2 4 54 43 1 1339 3 34 138 4 201 23 141 8 7 165 4 34 138 12 11 13 4 10 14 17 8 7 21 6 15 4 34 138 12 929 67 716 258 268 517 841 5 312 4 33 437 35 1041 23 798 1 23 1 267 30 45 4 1 640 1 33 381 285 997 1416 4 4 19 6 235 7 18 748 199 27 145 1275 12 4 1144 1 82 1 640 55 538 27 4 1162 1 798 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.260546 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.260793 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 1 5 9 16 22 31 43 44 57 65 66 67 79 88 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.360754 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 1 5 9 16 22 31 43 44 57 65 66 67 79 88 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 31 146 212 172 16 20 312 55 533 404 211 768 461 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.361152 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 31 146 212 172 16 20 312 55 533 404 211 768 461 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:02.361404 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:02.361531 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:02.363208 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] A ##4 ##S Essentia ##l G ##uid ##e to Valuations and Climate Change [MASK] [MASK] [SEP] limitations Cha ##l ##l ##en ##ge ##s So [MASK] ##ut ##ions and learnings Int [MASK] Aut ##h ##orit ##y and [MASK] ##pe [MASK] the framework [SEP]\n",
            "I0706 13:05:02.363456 140220668987264 create_pretraining_data.py:151] tokens: [CLS] A ##4 ##S Essentia ##l G ##uid ##e to Valuations and Climate Change [MASK] [MASK] [SEP] limitations Cha ##l ##l ##en ##ge ##s So [MASK] ##ut ##ions and learnings Int [MASK] Aut ##h ##orit ##y and [MASK] ##pe [MASK] the framework [SEP]\n",
            "INFO:tensorflow:input_ids: 2 12 11 13 16 10 14 17 8 7 21 6 15 20 4 4 3 281 1421 10 10 94 925 19 737 4 397 962 6 797 63 4 686 465 443 30 6 4 52 4 18 26 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.363968 140220668987264 create_pretraining_data.py:161] input_ids: 2 12 11 13 16 10 14 17 8 7 21 6 15 20 4 4 3 281 1421 10 10 94 925 19 737 4 397 962 6 797 63 4 686 465 443 30 6 4 52 4 18 26 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.364429 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.364861 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 14 15 25 31 37 39 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.365046 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 14 15 25 31 37 39 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 83 11 10 51 57 23 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.365218 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 83 11 10 51 57 23 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:02.365409 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:02.365555 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0706 13:05:02.366910 140220668987264 create_pretraining_data.py:149] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] Ł [SEP] [MASK] [SEP]\n",
            "I0706 13:05:02.367038 140220668987264 create_pretraining_data.py:151] tokens: [CLS] Ł [SEP] [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 2 5 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.367439 140220668987264 create_pretraining_data.py:161] input_ids: 2 5 3 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.367784 140220668987264 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.368114 140220668987264 create_pretraining_data.py:161] segment_ids: 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.460764 140220668987264 create_pretraining_data.py:161] masked_lm_positions: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 86 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0706 13:05:02.460913 140220668987264 create_pretraining_data.py:161] masked_lm_ids: 86 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0706 13:05:02.461011 140220668987264 create_pretraining_data.py:161] masked_lm_weights: 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0706 13:05:02.461080 140220668987264 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "INFO:tensorflow:Wrote 6906 total instances\n",
            "I0706 13:05:06.778450 140220668987264 create_pretraining_data.py:166] Wrote 6906 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5CqgJP9pnhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28935531-e47f-4539-d3ec-7dd588c7354e"
      },
      "source": [
        "BUCKET_NAME = \"esgbert_resourses\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"esgbert_model\" #@param {type:\"string\"}\n",
        "tf.gfile.MkDir(MODEL_DIR)\n",
        "!gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME\n",
        "if not BUCKET_NAME:\n",
        "  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n",
        "              \"You will not be able to train the model.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://pretraining_data/shard_0000.tfrecord [Content-Type=application/octet-stream]...\n",
            "/ [1/1 files][ 14.5 MiB/ 14.5 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/14.5 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFqIhceW7Bae",
        "outputId": "3a3d0717-c5f0-4841-d6b4-15d891358a29"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome! This command will take you through the configuration of gcloud.\n",
            "\n",
            "Settings from your current configuration [default] are:\n",
            "component_manager:\n",
            "  disable_update_check: 'True'\n",
            "compute:\n",
            "  gce_metadata_read_timeout_sec: '0'\n",
            "core:\n",
            "  account: srishtimehra@berkeley.edu\n",
            "\n",
            "Pick configuration to use:\n",
            " [1] Re-initialize this configuration [default] with new settings \n",
            " [2] Create a new configuration\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "Your current configuration has been set to: [default]\n",
            "\n",
            "You can skip diagnostics next time by using the following flag:\n",
            "  gcloud init --skip-diagnostics\n",
            "\n",
            "Network diagnostic detects and fixes local network connection issues.\n",
            "Reachability Check passed.\n",
            "Network diagnostic passed (1/1 checks passed).\n",
            "\n",
            "Choose the account you would like to use to perform operations for \n",
            "this configuration:\n",
            " [1] srishtimehra@berkeley.edu\n",
            " [2] Log in with a new account\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "You are logged in as: [srishtimehra@berkeley.edu].\n",
            "\n",
            "Pick cloud project to use: \n",
            " [1] w266-313108\n",
            " [2] Create a new project\n",
            "Please enter numeric choice or text value (must exactly match list \n",
            "item):  1\n",
            "\n",
            "Your current project has been set to: [w266-313108].\n",
            "\n",
            "Do you want to configure a default Compute Region and Zone? (Y/n)?  n\n",
            "\n",
            "Your Google Cloud SDK is configured and ready to use!\n",
            "\n",
            "* Commands that require authentication will use srishtimehra@berkeley.edu by default\n",
            "* Commands will reference project `w266-313108` by default\n",
            "Run `gcloud help config` to learn how to change individual settings\n",
            "\n",
            "This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.\n",
            "Run `gcloud topic configurations` to learn more.\n",
            "\n",
            "Some things to try next:\n",
            "\n",
            "* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.\n",
            "* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM_G5xp_pyW3"
      },
      "source": [
        "bert_base_config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1, \n",
        "  \"directionality\": \"bidi\", \n",
        "  \"hidden_act\": \"gelu\", \n",
        "  \"hidden_dropout_prob\": 0.1, \n",
        "  \"hidden_size\": 768, \n",
        "  \"initializer_range\": 0.02, \n",
        "  \"intermediate_size\": 3072, \n",
        "  \"max_position_embeddings\": 512, \n",
        "  \"num_attention_heads\": 12, \n",
        "  \"num_hidden_layers\": 12, \n",
        "  \"pooler_fc_size\": 768, \n",
        "  \"pooler_num_attention_heads\": 12, \n",
        "  \"pooler_num_fc_layers\": 3, \n",
        "  \"pooler_size_per_head\": 128, \n",
        "  \"pooler_type\": \"first_token_transform\", \n",
        "  \"type_vocab_size\": 2, \n",
        "  \"vocab_size\": VOC_SIZE\n",
        "}\n",
        "\n",
        "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
        "  json.dump(bert_base_config, fo, indent=2)\n",
        "  \n",
        "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_shoX1vp5Q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36dcd897-b5e7-4ff3-fee7-31dd1f53e192"
      },
      "source": [
        "if BUCKET_NAME:\n",
        "  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://esgbert_model/bert_config.json [Content-Type=application/json]...\n",
            "/ [0/3 files][    0.0 B/ 14.5 MiB]   0% Done                                    \rCopying file://esgbert_model/esgvocab.txt [Content-Type=text/plain]...\n",
            "Copying file://pretraining_data/shard_0000.tfrecord [Content-Type=application/octet-stream]...\n",
            "/ [3/3 files][ 14.5 MiB/ 14.5 MiB] 100% Done                                    \n",
            "Operation completed over 3 objects/14.5 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MnSdIBQ7nxC",
        "outputId": "ae596b4e-b97c-4064-dd65-f0a67b61c6ef"
      },
      "source": [
        "BUCKET_NAME = \"esgbert_resourses\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"esgbert_model\" #@param {type:\"string\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
        "VOC_FNAME = \"esgvocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "# Input data pipeline config\n",
        "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 75 #@param {type:\"integer\"}\n",
        "MAX_SEQ_LENGTH = 512 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "\n",
        "# Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "TRAIN_STEPS = 100 #@param {type:\"integer\"}\n",
        "SAVE_CHECKPOINTS_STEPS = 25 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "if BUCKET_NAME:\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "else:\n",
        "  BUCKET_PATH = \".\"\n",
        "\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
        "\n",
        "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
        "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
        "\n",
        "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-06 13:06:38,646 :  Using checkpoint: None\n",
            "2021-07-06 13:06:38,649 :  Using 1 data shards\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jswj9pSi8bsg",
        "outputId": "4dcd039a-9e36-49ed-dc6c-646d6e4c09f5"
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=TRAIN_STEPS,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=True)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=BERT_GCS_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "#run_config = tf.estimator.RunConfig(\n",
        " #   model_dir=BERT_GCS_DIR,\n",
        "  #  save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "#estimator = tf.estimator.Estimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)\n",
        "  \n",
        "train_input_fn = input_fn_builder(\n",
        "        input_files=input_files,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "        is_training=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-06 13:06:49,158 :  Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f0e85831050>) includes params argument, but params are not passed to Estimator.\n",
            "2021-07-06 13:06:49,164 :  Using config: {'_model_dir': 'gs://esgbert_resourses/esgbert_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 25, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.18.117.138:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0e85647e90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.18.117.138:8470', '_evaluation_master': 'grpc://10.18.117.138:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=25, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f0e85647910>}\n",
            "2021-07-06 13:06:49,168 :  _TPUContext: eval_on_tpu True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXo_Bl0ND_CQ",
        "outputId": "7992da5b-1d8c-4bad-ee71-38ee5b0c4b76"
      },
      "source": [
        "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-06 13:06:53,216 :  Querying Tensorflow master (grpc://10.18.117.138:8470) for TPU system metadata.\n",
            "2021-07-06 13:06:53,227 :  Found TPU system:\n",
            "2021-07-06 13:06:53,228 :  *** Num TPU Cores: 8\n",
            "2021-07-06 13:06:53,235 :  *** Num TPU Workers: 1\n",
            "2021-07-06 13:06:53,236 :  *** Num TPU Cores Per Worker: 8\n",
            "2021-07-06 13:06:53,247 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 15916867275321068859)\n",
            "2021-07-06 13:06:53,250 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 18062280619181287568)\n",
            "2021-07-06 13:06:53,251 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4625725992863806867)\n",
            "2021-07-06 13:06:53,253 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9077909587501769289)\n",
            "2021-07-06 13:06:53,254 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10924542538409281425)\n",
            "2021-07-06 13:06:53,256 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2207136648589515619)\n",
            "2021-07-06 13:06:53,257 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 10636179430782215954)\n",
            "2021-07-06 13:06:53,259 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17107039456643358989)\n",
            "2021-07-06 13:06:53,262 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 12758500462899703959)\n",
            "2021-07-06 13:06:53,266 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5225307900916312222)\n",
            "2021-07-06 13:06:53,268 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16318467796201394237)\n",
            "2021-07-06 13:06:53,289 :  From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "2021-07-06 13:06:53,292 :  From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "2021-07-06 13:06:53,313 :  Calling model_fn.\n",
            "2021-07-06 13:06:53,315 :  From /content/bert/run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "2021-07-06 13:06:53,329 :  From /content/bert/run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "2021-07-06 13:06:53,331 :  From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "2021-07-06 13:06:53,375 :  From /content/bert/run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "2021-07-06 13:06:53,376 :  From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "2021-07-06 13:06:53,456 :  From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "2021-07-06 13:06:53,580 :  From /content/bert/run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2021-07-06 13:06:53,618 :  Found small feature: next_sentence_labels [16, 1]\n",
            "2021-07-06 13:06:53,623 :  Found small feature: next_sentence_labels [16, 1]\n",
            "2021-07-06 13:06:53,630 :  Found small feature: next_sentence_labels [16, 1]\n",
            "2021-07-06 13:06:53,636 :  Found small feature: next_sentence_labels [16, 1]\n",
            "2021-07-06 13:06:53,641 :  Found small feature: next_sentence_labels [16, 1]\n",
            "2021-07-06 13:06:53,648 :  Found small feature: next_sentence_labels [16, 1]\n",
            "2021-07-06 13:06:53,655 :  Found small feature: next_sentence_labels [16, 1]\n",
            "2021-07-06 13:06:53,661 :  Found small feature: next_sentence_labels [16, 1]\n",
            "2021-07-06 13:06:53,711 :  From /content/bert/run_pretraining.py:117: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "2021-07-06 13:06:53,715 :  *** Features ***\n",
            "2021-07-06 13:06:53,717 :    name = input_ids, shape = (16, 512)\n",
            "2021-07-06 13:06:53,720 :    name = input_mask, shape = (16, 512)\n",
            "2021-07-06 13:06:53,722 :    name = masked_lm_ids, shape = (16, 75)\n",
            "2021-07-06 13:06:53,727 :    name = masked_lm_positions, shape = (16, 75)\n",
            "2021-07-06 13:06:53,734 :    name = masked_lm_weights, shape = (16, 75)\n",
            "2021-07-06 13:06:53,736 :    name = next_sentence_labels, shape = (16, 1)\n",
            "2021-07-06 13:06:53,739 :    name = segment_ids, shape = (16, 512)\n",
            "2021-07-06 13:06:53,742 :  From bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "2021-07-06 13:06:53,756 :  From bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "2021-07-06 13:06:53,805 :  From bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "2021-07-06 13:06:53,888 :  From bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "2021-07-06 13:06:53,914 :  From bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "2021-07-06 13:06:53,918 :  From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "2021-07-06 13:06:57,060 :  From /content/bert/run_pretraining.py:150: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "2021-07-06 13:06:57,062 :  **** Trainable Variables ****\n",
            "2021-07-06 13:06:57,065 :    name = bert/embeddings/word_embeddings:0, shape = (2000, 768)\n",
            "2021-07-06 13:06:57,068 :    name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "2021-07-06 13:06:57,070 :    name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "2021-07-06 13:06:57,071 :    name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,072 :    name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,074 :    name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,075 :    name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,076 :    name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,077 :    name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,078 :    name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,079 :    name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,081 :    name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,082 :    name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,083 :    name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,084 :    name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,085 :    name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,086 :    name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,088 :    name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,089 :    name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,090 :    name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,091 :    name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,092 :    name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,094 :    name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,095 :    name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,096 :    name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,097 :    name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,098 :    name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,099 :    name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,101 :    name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,102 :    name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,103 :    name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,105 :    name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,106 :    name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,107 :    name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,108 :    name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,110 :    name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,111 :    name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,112 :    name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,113 :    name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,115 :    name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,116 :    name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,117 :    name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,118 :    name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,119 :    name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,120 :    name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,122 :    name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,123 :    name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,124 :    name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,125 :    name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,126 :    name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,127 :    name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,129 :    name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,130 :    name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,131 :    name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,132 :    name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,133 :    name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,134 :    name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,136 :    name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,137 :    name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,138 :    name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,139 :    name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,140 :    name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,142 :    name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,143 :    name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,144 :    name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,145 :    name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,146 :    name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,148 :    name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,149 :    name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,150 :    name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,151 :    name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,479 :    name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,483 :    name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,492 :    name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,494 :    name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,497 :    name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,500 :    name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,501 :    name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,503 :    name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,505 :    name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,508 :    name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,509 :    name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,512 :    name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,514 :    name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,517 :    name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,520 :    name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,523 :    name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,525 :    name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,528 :    name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,533 :    name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,535 :    name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,537 :    name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,539 :    name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,546 :    name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,552 :    name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,554 :    name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,559 :    name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,560 :    name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,562 :    name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,563 :    name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,565 :    name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,566 :    name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,567 :    name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,568 :    name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,569 :    name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,571 :    name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,572 :    name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,573 :    name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,574 :    name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,575 :    name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,578 :    name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,579 :    name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,586 :    name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,587 :    name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,591 :    name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,593 :    name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,594 :    name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,595 :    name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,597 :    name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,598 :    name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,599 :    name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,600 :    name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,602 :    name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,603 :    name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,604 :    name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,606 :    name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,607 :    name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,608 :    name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,610 :    name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,611 :    name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,612 :    name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,613 :    name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,615 :    name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,616 :    name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,617 :    name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,618 :    name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,620 :    name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,621 :    name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,622 :    name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,623 :    name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,624 :    name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,625 :    name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,627 :    name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,628 :    name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,629 :    name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,630 :    name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,631 :    name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,633 :    name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,634 :    name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,635 :    name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,636 :    name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,638 :    name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,639 :    name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,641 :    name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,643 :    name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,644 :    name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,645 :    name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,646 :    name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,647 :    name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,648 :    name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,649 :    name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,650 :    name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,652 :    name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,653 :    name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,654 :    name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,655 :    name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,656 :    name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,657 :    name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,658 :    name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,660 :    name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,661 :    name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,662 :    name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,663 :    name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,665 :    name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,666 :    name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,667 :    name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,668 :    name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,670 :    name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,671 :    name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,672 :    name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,673 :    name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,674 :    name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,676 :    name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,677 :    name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,678 :    name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,679 :    name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,680 :    name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,682 :    name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,683 :    name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,684 :    name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,685 :    name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,686 :    name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2021-07-06 13:06:57,688 :    name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "2021-07-06 13:06:57,689 :    name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "2021-07-06 13:06:57,690 :    name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,692 :    name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,693 :    name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,694 :    name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,695 :    name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,697 :    name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "2021-07-06 13:06:57,698 :    name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "2021-07-06 13:06:57,699 :    name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "2021-07-06 13:06:57,700 :    name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "2021-07-06 13:06:57,702 :    name = cls/predictions/output_bias:0, shape = (2000,)\n",
            "2021-07-06 13:06:57,703 :    name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "2021-07-06 13:06:57,705 :    name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "2021-07-06 13:06:57,706 :  From bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "2021-07-06 13:06:57,710 :  From bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "2021-07-06 13:06:58,314 :  From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2021-07-06 13:07:10,459 :  Create CheckpointSaverHook.\n",
            "2021-07-06 13:07:10,841 :  Done calling model_fn.\n",
            "2021-07-06 13:07:13,423 :  TPU job name worker\n",
            "2021-07-06 13:07:14,796 :  Graph was finalized.\n",
            "2021-07-06 13:07:19,280 :  Running local_init_op.\n",
            "2021-07-06 13:07:19,538 :  Done running local_init_op.\n",
            "2021-07-06 13:07:27,374 :  Saving checkpoints for 0 into gs://esgbert_resourses/esgbert_model/model.ckpt.\n",
            "2021-07-06 13:07:39,298 :  From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:751: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "2021-07-06 13:07:39,934 :  Initialized dataset iterators in 0 seconds\n",
            "2021-07-06 13:07:39,939 :  Installing graceful shutdown hook.\n",
            "2021-07-06 13:07:39,945 :  Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "2021-07-06 13:07:39,959 :  Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "2021-07-06 13:07:39,967 :  Init TPU system\n",
            "2021-07-06 13:07:51,429 :  Initialized TPU in 11 seconds\n",
            "2021-07-06 13:07:51,432 :  Starting infeed thread controller.\n",
            "2021-07-06 13:07:51,438 :  Starting outfeed thread controller.\n",
            "2021-07-06 13:07:51,780 :  Enqueue next (25) batch(es) of data to infeed.\n",
            "2021-07-06 13:07:51,782 :  Dequeue next (25) batch(es) of data from outfeed.\n",
            "2021-07-06 13:08:36,491 :  Outfeed finished for iteration (0, 0)\n",
            "2021-07-06 13:08:45,724 :  Saving checkpoints for 25 into gs://esgbert_resourses/esgbert_model/model.ckpt.\n",
            "2021-07-06 13:08:58,535 :  loss = 7.5816236, step = 25\n",
            "2021-07-06 13:08:58,540 :  Enqueue next (25) batch(es) of data to infeed.\n",
            "2021-07-06 13:08:58,544 :  Dequeue next (25) batch(es) of data from outfeed.\n",
            "2021-07-06 13:09:12,202 :  Saving checkpoints for 50 into gs://esgbert_resourses/esgbert_model/model.ckpt.\n",
            "2021-07-06 13:09:25,899 :  global_step/sec: 0.913738\n",
            "2021-07-06 13:09:25,901 :  examples/sec: 116.958\n",
            "2021-07-06 13:09:25,905 :  Enqueue next (25) batch(es) of data to infeed.\n",
            "2021-07-06 13:09:25,909 :  Dequeue next (25) batch(es) of data from outfeed.\n",
            "2021-07-06 13:09:36,227 :  Saving checkpoints for 75 into gs://esgbert_resourses/esgbert_model/model.ckpt.\n",
            "2021-07-06 13:09:48,582 :  global_step/sec: 1.10212\n",
            "2021-07-06 13:09:48,586 :  examples/sec: 141.071\n",
            "2021-07-06 13:09:48,590 :  Enqueue next (25) batch(es) of data to infeed.\n",
            "2021-07-06 13:09:48,594 :  Dequeue next (25) batch(es) of data from outfeed.\n",
            "2021-07-06 13:09:49,684 :  Outfeed finished for iteration (3, 0)\n",
            "2021-07-06 13:09:58,904 :  Saving checkpoints for 100 into gs://esgbert_resourses/esgbert_model/model.ckpt.\n",
            "2021-07-06 13:10:12,274 :  global_step/sec: 1.05523\n",
            "2021-07-06 13:10:12,276 :  examples/sec: 135.069\n",
            "2021-07-06 13:10:12,697 :  Stop infeed thread controller\n",
            "2021-07-06 13:10:12,699 :  Shutting down InfeedController thread.\n",
            "2021-07-06 13:10:12,706 :  InfeedController received shutdown signal, stopping.\n",
            "2021-07-06 13:10:12,710 :  Infeed thread finished, shutting down.\n",
            "2021-07-06 13:10:12,717 :  infeed marked as finished\n",
            "2021-07-06 13:10:12,724 :  Stop output thread controller\n",
            "2021-07-06 13:10:12,726 :  Shutting down OutfeedController thread.\n",
            "2021-07-06 13:10:12,729 :  OutfeedController received shutdown signal, stopping.\n",
            "2021-07-06 13:10:12,731 :  Outfeed thread finished, shutting down.\n",
            "2021-07-06 13:10:12,732 :  outfeed marked as finished\n",
            "2021-07-06 13:10:12,733 :  Shutdown TPU system.\n",
            "2021-07-06 13:10:13,443 :  Loss for final step: 6.1370573.\n",
            "2021-07-06 13:10:13,445 :  training_loop marked as finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.tpu.tpu_estimator.TPUEstimator at 0x7f0e8564be90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "HQ6tEno18frD",
        "outputId": "3a448efe-df84-407d-e083-e9b1f71168b3"
      },
      "source": [
        "estimator.export_saved_model(MODEL_DIR)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-68d77c4cfaa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: export_saved_model() missing 1 required positional argument: 'serving_input_receiver_fn'"
          ]
        }
      ]
    }
  ]
}